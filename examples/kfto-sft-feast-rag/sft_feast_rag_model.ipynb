{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "5da333c8bd896040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install the YAML magic\n",
    "%pip install yamlmagic\n",
    "%load_ext yamlmagic"
   ],
   "id": "304ca8980a46c8a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install the Kubeflow SDK (this can be removed once the latest version is included into workbench images)\n",
    "%pip install git+https://github.com/kubeflow/trainer.git@release-1.9#subdirectory=sdk/python"
   ],
   "id": "57d0173afea736ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Configuration\n",
    "Edit the following training parameters:"
   ],
   "id": "3642a51d6fa353a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Model\n",
    "model_name_or_path: facebook/bart-base    # only works with Encoder-Decoder models\n",
    "model_revision: main\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: eager                # one of eager (default), sdpa or flash_attention_2\n",
    "use_liger: false                          # use Liger kernels\n",
    "\n",
    "# PEFT / LoRA (Apply to Generator Model)\n",
    "use_peft: false\n",
    "lora_r: 16\n",
    "lora_alpha: 8\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]   # Ensure these match your generator model\n",
    "lora_modules_to_save: []\n",
    "\n",
    "# QLoRA (BitsAndBytes) (Apply to Generator Model)\n",
    "load_in_4bit: false                       # use 4 bit precision for the base model (only with LoRA)\n",
    "load_in_8bit: false                       # use 8 bit precision for the base model (only with LoRA)\n",
    "\n",
    "# Dataset\n",
    "dataset_name: facebook/wiki_dpr\n",
    "dataset_config: main                      # name of the dataset configuration\n",
    "dataset_train_split: train                # dataset split to use for training (for RAG generated data)\n",
    "dataset_test_split: test                  # dataset split to use for evaluation (for RAG generated data)\n",
    "dataset_kwargs:\n",
    "    add_special_tokens: false               # template with special tokens\n",
    "    append_concat_token: false              # add additional separator token\n",
    "\n",
    "# SFT (These parameters will now apply to the RagModel's training)\n",
    "max_seq_length: 1024                      # max sequence length for model and packing of the dataset\n",
    "dataset_batch_size: 1000                  # samples to tokenize per batch (for initial data processing)\n",
    "packing: false                            # Packing is generally not used directly with RagModel training in the same way as SFT\n",
    "\n",
    "# Training\n",
    "num_train_epochs: 3                       # number of training epochs\n",
    "remove_unused_columns: false\n",
    "label_smoothing_factor: 0.1\n",
    "\n",
    "per_device_train_batch_size: 1            # Batch size per device during training\n",
    "per_device_eval_batch_size: 1             # Batch size for evaluation\n",
    "auto_find_batch_size: false               # find a batch size that fits into memory automatically\n",
    "eval_strategy: epoch                      # evaluate every epoch\n",
    "\n",
    "bf16: true                                # use bf16 16-bit (mixed) precision\n",
    "tf32: true                               # use tf32 precision\n",
    "\n",
    "learning_rate: 4.0e-6                     # Initial learning rate for RAG model training\n",
    "warmup_steps: 150                         # steps for a linear warmup from 0 to `learning_rate`\n",
    "lr_scheduler_type: cosine                 # learning rate scheduler (see transformers.SchedulerType)\n",
    "\n",
    "optim: adamw_torch_fused                  # optimizer (see transformers.OptimizerNames)\n",
    "max_grad_norm: 1.0                        # max gradient norm\n",
    "seed: 42\n",
    "\n",
    "gradient_accumulation_steps: 8            # Increase for smaller per_device_train_batch_size\n",
    "gradient_checkpointing: false             # use gradient checkpointing to save memory\n",
    "gradient_checkpointing_kwargs:\n",
    "    use_reentrant: false\n",
    "\n",
    "# FSDP\n",
    "fsdp: \"full_shard auto_wrap\"              # add offload if not enough GPU memory\n",
    "fsdp_config:\n",
    "    activation_checkpointing: true\n",
    "    cpu_ram_efficient_loading: false\n",
    "    sync_module_states: true\n",
    "    use_orig_params: true\n",
    "    limit_all_gathers: false\n",
    "\n",
    "# Checkpointing\n",
    "save_strategy: epoch                      # save checkpoint every epoch\n",
    "save_total_limit: 1                       # limit the total amount of checkpoints\n",
    "resume_from_checkpoint: false             # load the last checkpoint in output_dir and resume from it\n",
    "\n",
    "# Logging\n",
    "log_level: warning                        # logging level (see transformers.logging)\n",
    "logging_strategy: steps\n",
    "logging_steps: 1                          # log every N steps\n",
    "report_to:\n",
    "- tensorboard                             # report metrics to tensorboard\n",
    "\n",
    "output_dir: /mnt/shared/fine_tuned_rag_model"
   ],
   "id": "3091b1ffed0be2d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feast Setup",
   "id": "24ff2ad5e7b6d650"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "%%bash\npip install --quiet feast[milvus] sentence-transformers datasets faiss-cpu\npip install bigtree==0.19.2\npip install marshmallow==3.10.0\npip install git+https://github.com/feast-dev/feast.git@master",
   "id": "ae9592e69cd4b0c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from datasets import load_dataset\n# load wikipedia dataset - 1% of the training split\ndataset = load_dataset(\n    \"facebook/wiki_dpr\",\n    \"psgs_w100.nq.exact\",\n    split=\"train[:1%]\",\n    with_index=False,\n    trust_remote_code=True\n)",
   "id": "41b1b6802c35bded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def chunk_dataset(examples, chunk_size=100, overlap=20, max_chars=100):\n    all_chunks = []\n    all_ids = []\n    all_titles = []\n\n    for i, text in enumerate(examples['text']):  # Iterate over texts in the batch\n        words = text.split()\n        chunks = []\n        for j in range(0, len(words), chunk_size - overlap):\n            chunk_words = words[j:j + chunk_size]\n            if len(chunk_words) < 20:\n                continue\n            chunk_text_value = ' '.join(chunk_words)  # Store the chunk text\n            if len(chunk_text_value) > max_chars:\n                chunk_text_value = chunk_text_value[:max_chars]\n            chunks.append(chunk_text_value)\n            all_ids.append(f\"{examples['id'][i]}_{j}\")  # Unique ID for the chunk\n            all_titles.append(examples['title'][i])\n\n        all_chunks.extend(chunks)\n\n    return {'id': all_ids, 'title': all_titles, 'text': all_chunks}\n\n\nchunked_dataset = dataset.map(\n    chunk_dataset,\n    batched=True,\n    remove_columns=dataset.column_names,\n    num_proc=1\n)",
   "id": "9daa7649b8666b4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\n\nsentences = chunked_dataset[\"text\"]\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = embedding_model.encode(sentences, show_progress_bar=True, batch_size=64, device=\"cuda\")\n\nprint(f\"Generated embeddings of shape: {embeddings.shape}\")",
   "id": "2b402115ffb6624f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"passage_id\": list(range(len(sentences))),\n",
    "    \"passage_text\": sentences,\n",
    "    \"embedding\": pd.Series(\n",
    "        [embedding.tolist() for embedding in embeddings],\n",
    "        dtype=object\n",
    "    ),\n",
    "    \"event_timestamp\": [datetime.now(timezone.utc) for _ in sentences],\n",
    "})\n",
    "\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.head())\n",
    "print(df[\"embedding\"].apply(lambda x: len(x) if isinstance(x, list) else str(type(x))).value_counts())  # Check lengths\n",
    "\n",
    "# Save to Parquet\n",
    "df.to_parquet(\"feature_repo/wiki_dpr.parquet\", index=False)\n",
    "print(\"Saved to wiki_dpr.parquet\")"
   ],
   "id": "8fd84128d583b28e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "%cd feature_repo",
   "id": "413890266f8612fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%feast apply",
   "id": "9d0aca5f4b325d9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "df = pd.read_parquet(\"./wiki_dpr.parquet\")\n",
    "chunk_size = 10000\n",
    "num_rows = len(df)\n",
    "\n",
    "for i in range(0, num_rows, chunk_size):\n",
    "    chunk_df = df.iloc[i:i + chunk_size]\n",
    "    print(f\"Writing chunk {i//chunk_size + 1}/{(num_rows + chunk_size - 1)//chunk_size}...\")\n",
    "    store.write_to_online_store(feature_view_name='wiki_passages', df=chunk_df)\n",
    "    print(\"Chunk written successfully.\")\n",
    "\n",
    "print(\"All data written to online store.\")"
   ],
   "id": "cbbfa1ae4c3af6f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Loop\n",
    "\n",
    "Review the training function. You can adjust the chat template if needed depending on the model you want to fine-tune:"
   ],
   "id": "cdbd2fc54c206eb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy over the `kfto-sft-feast-rag` folder into the shared storage, so that it can be accessed by the training job, since the FeastRagRetriever requires the `feature_repo` directory during training and inference. Copy over the `feast_rag_retriever.py` script as well, so that it can be imported in the training job.",
   "id": "33d04d5153b13b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%bash\n",
    "cp -r $HOME/distributed-workloads/examples/kfto-sft-feast-rag $HOME/shared/\n",
    "cp $HOME/distributed-workloads/examples/kfto_feast_rag/feast_rag_retriever.py $HOME/shared/kfto-sft-feast-rag/"
   ],
   "id": "d2b869fb1399fb29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(parameters):\n",
    "    import subprocess, sys\n",
    "    # --- Install necessary packages ---\n",
    "    # This ensures Feast and other dependencies are available inside the container\n",
    "    print(\"Installing Feast and other RAG dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"feast[milvus]\", \"sentence-transformers\", \"datasets\", \"bigtree==0.19.2\", \"marshmallow==3.10.0\", \"protobuf>=3.10.0\", \"git+https://github.com/feast-dev/feast.git@master\", \"faiss-cpu\"])\n",
    "    print(\"Feast and other RAG dependencies installed.\")\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import random\n",
    "    from datasets import Dataset\n",
    "    from transformers import (\n",
    "        set_seed,\n",
    "        RagSequenceForGeneration,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        RagTokenizer,\n",
    "        default_data_collator,\n",
    "        RagConfig,\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        AutoModel,\n",
    "    )\n",
    "    from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "    import torch\n",
    "\n",
    "    from trl import (\n",
    "        ModelConfig,\n",
    "        ScriptArguments,\n",
    "        SFTConfig,\n",
    "        TrlParser,\n",
    "        get_peft_config,\n",
    "    )\n",
    "\n",
    "    # This is required for `feast_rag_retriever` to be found (This is temporary until Feast RAG Retriever is part of Feast SDK)\n",
    "    CUSTOM_MODULES_PATH = \"/mnt/shared/kfto-sft-feast-rag\"\n",
    "    sys.path.append(CUSTOM_MODULES_PATH)\n",
    "    print(f\"Added {CUSTOM_MODULES_PATH} to sys.path for custom module imports.\")\n",
    "    from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "    from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args_trl, model_args = parser.parse_dict(parameters)\n",
    "\n",
    "    # Convert SFTConfig parameters to standard TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=training_args_trl.output_dir,\n",
    "        num_train_epochs=training_args_trl.num_train_epochs,\n",
    "        per_device_train_batch_size=training_args_trl.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_args_trl.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=training_args_trl.gradient_accumulation_steps,\n",
    "        gradient_checkpointing=training_args_trl.gradient_checkpointing,\n",
    "        learning_rate=training_args_trl.learning_rate,\n",
    "        warmup_steps=training_args_trl.warmup_steps,\n",
    "        lr_scheduler_type=training_args_trl.lr_scheduler_type,\n",
    "        optim=training_args_trl.optim,\n",
    "        max_grad_norm=training_args_trl.max_grad_norm,\n",
    "        seed=training_args_trl.seed,\n",
    "        bf16=training_args_trl.bf16,\n",
    "        tf32=training_args_trl.tf32,\n",
    "        eval_strategy=training_args_trl.eval_strategy,\n",
    "        save_strategy=training_args_trl.save_strategy,\n",
    "        save_total_limit=training_args_trl.save_total_limit,\n",
    "        logging_strategy=training_args_trl.logging_strategy,\n",
    "        logging_steps=training_args_trl.logging_steps,\n",
    "        report_to=training_args_trl.report_to,\n",
    "        fsdp=training_args_trl.fsdp if hasattr(training_args_trl, 'fsdp') else None,\n",
    "        fsdp_config=training_args_trl.fsdp_config if hasattr(training_args_trl, 'fsdp_config') else None,\n",
    "        resume_from_checkpoint=training_args_trl.resume_from_checkpoint,\n",
    "        remove_unused_columns=training_args_trl.remove_unused_columns,\n",
    "    )\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # --- Load Models for RAG ---\n",
    "    # Question Encoder\n",
    "    question_encoder_model_name_or_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    question_encoder_tokenizer = AutoTokenizer.from_pretrained(question_encoder_model_name_or_path, trust_remote_code=model_args.trust_remote_code)\n",
    "    question_encoder_model = AutoModel.from_pretrained(\n",
    "        question_encoder_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        torch_dtype=model_args.torch_dtype\n",
    "    )\n",
    "\n",
    "    # Generator Model to be fine-tuned\n",
    "    generator_model_name_or_path = parameters[\"model_name_or_path\"]\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        use_fast=True\n",
    "    )\n",
    "\n",
    "    if generator_tokenizer.pad_token is None:\n",
    "        if generator_tokenizer.eos_token is not None:\n",
    "            print(f\"Generator tokenizer pad_token not set. Using eos_token ({generator_tokenizer.eos_token}) as pad_token.\")\n",
    "            generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "        else:\n",
    "            print(\"Warning: Generator tokenizer has no pad_token or eos_token. Adding a new pad token.\")\n",
    "            generator_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=model_args.torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    )\n",
    "\n",
    "    if model_args.use_peft:\n",
    "        print(f\"PEFT/LoRA: Ensure 'lora_target_modules' in your parameters ({parameters.get('lora_target_modules')}) are correct for the new generator model '{generator_model_name_or_path}'.\")\n",
    "        generator_model = prepare_model_for_kbit_training(generator_model)\n",
    "        peft_config = get_peft_config(model_args)\n",
    "        generator_model = get_peft_model(generator_model, peft_config)\n",
    "        print(\"PEFT setup for generator model completed.\")\n",
    "\n",
    "    store_path = CUSTOM_MODULES_PATH+\"/feature_repo\"\n",
    "\n",
    "    # --- Initialize FeastRAGRetriever ---\n",
    "    feast_index = FeastIndex()\n",
    "    rag_top_k = 5\n",
    "\n",
    "    question_encoder_config = {\n",
    "        \"model_type\": \"dpr\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"vocab_size\": question_encoder_tokenizer.vocab_size,\n",
    "        \"num_hidden_layers\": 6,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"projection_dim\": 0,\n",
    "        \"torch_dtype\": model_args.torch_dtype\n",
    "    }\n",
    "\n",
    "    rag_config = RagConfig(\n",
    "        question_encoder=question_encoder_config,\n",
    "        generator=generator_model.config.to_dict(),\n",
    "        index_name=\"custom\",\n",
    "        index={\"index_name\": \"feast_dummy_index\", \"custom_type\": \"FeastIndex\"}\n",
    "    )\n",
    "\n",
    "    features_to_retrieve = [\n",
    "        \"wiki_passages:passage_text\",\n",
    "        \"wiki_passages:embedding\",\n",
    "        \"wiki_passages:passage_id\",\n",
    "    ]\n",
    "\n",
    "    rag_retriever = FeastRAGRetriever(\n",
    "        question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "        question_encoder=question_encoder_model,\n",
    "        generator_tokenizer=generator_tokenizer,\n",
    "        generator_model=generator_model,\n",
    "        feast_repo_path=store_path,\n",
    "        feature_view=wiki_passage_feature_view,\n",
    "        features=features_to_retrieve,\n",
    "        search_type=\"vector\",\n",
    "        config=rag_config,\n",
    "        index=feast_index,\n",
    "        query_encoder_model=\"all-MiniLM-L6-v2\",\n",
    "    )\n",
    "\n",
    "    # --- Initialize the RagModel for fine-tuning ---\n",
    "    model = RagSequenceForGeneration(\n",
    "        config=rag_config,\n",
    "        generator=generator_model,\n",
    "        retriever=rag_retriever\n",
    "    )\n",
    "    print(\"RAG Model initialized.\")\n",
    "\n",
    "    # Explicitly move model to GPU\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # --- Dataset Generation for Fine-tuning ---\n",
    "    print(\"Preparing training data (Q&A pairs) with caching...\")\n",
    "    # Cache directory for synthetic data generated in a previous run to save time\n",
    "    synthetic_data_cache_dir = \"/mnt/shared/synthetic_data_cache/rag_synthetic_qa_dataset\"\n",
    "    Path(synthetic_data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    cached_dataset_path = os.path.join(synthetic_data_cache_dir, \"rag_synthetic_qa_dataset\")\n",
    "\n",
    "    if Path(cached_dataset_path).exists():\n",
    "        print(f\"Loading synthetic data from cache: {cached_dataset_path}\")\n",
    "        synthetic_dataset = Dataset.load_from_disk(cached_dataset_path)\n",
    "    else:\n",
    "        print(\"Generating training data for RAG fine-tuning (Q&A pairs)...\")\n",
    "        synthetic_data = []\n",
    "        try:\n",
    "            with open(\"/mnt/shared/kfto-sft-feast-rag/generated_questions.txt\", 'r') as file:\n",
    "                example_queries = [line.strip().strip(',\"') for line in file if line.strip()]\n",
    "        except Exception as e:\n",
    "            print(\"Error reading 'generated_questions.txt'\")\n",
    "            raise e\n",
    "\n",
    "        for i, query in enumerate(example_queries):\n",
    "            print(f\"Generating answer for query {i+1}/{len(example_queries)}: '{query}'\")\n",
    "            try:\n",
    "                rag_answer = rag_retriever.generate_answer(query, top_k=rag_top_k, max_new_tokens=380)\n",
    "                if not rag_answer or not rag_answer.strip():\n",
    "                    print(f\"Skipping empty answer for query: '{query}'\")\n",
    "                    continue\n",
    "                print(f\"Generated answer: {rag_answer}\")\n",
    "                # Tokenize question for the RAG model's question encoder\n",
    "                tokenized_question = question_encoder_tokenizer(\n",
    "                    query,\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"np\",\n",
    "                )\n",
    "\n",
    "                # Tokenize answer for the RAG model's generator (as labels)\n",
    "                tokenized_answer_for_labels = generator_tokenizer(\n",
    "                    text_target=rag_answer,\n",
    "                    truncation=True,\n",
    "                    max_length=32,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"np\",\n",
    "                )\n",
    "\n",
    "                synthetic_data.append({\n",
    "                    \"input_ids\": tokenized_question[\"input_ids\"].flatten().tolist(),\n",
    "                    \"attention_mask\": tokenized_question[\"attention_mask\"].flatten().tolist(),\n",
    "                    \"labels\": tokenized_answer_for_labels[\"input_ids\"].flatten().tolist(),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating answer for query '{query}': {e}. Skipping this query.\")\n",
    "\n",
    "        synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "        print(f\"Synthetic data generation complete. Saving to cache: {cached_dataset_path}\")\n",
    "        synthetic_dataset.save_to_disk(cached_dataset_path)\n",
    "        print(\"Synthetic data saved to cache.\")\n",
    "\n",
    "    train_dataset = synthetic_dataset.train_test_split(test_size=0.1, seed=training_args.seed)[script_args.dataset_train_split]\n",
    "    test_dataset = synthetic_dataset.train_test_split(test_size=0.1, seed=training_args.seed)[script_args.dataset_test_split]\n",
    "\n",
    "    with training_args.main_process_first(desc=\"Log few samples from the training set\"):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(f\"Sample Input IDs: {train_dataset[index]['input_ids'][:50]}...\")\n",
    "            print(f\"Sample Labels: {train_dataset[index]['labels'][:50]}...\")\n",
    "\n",
    "    # --- Training the RagModel ---\n",
    "    from transformers import DataCollatorWithPadding\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=generator_tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        if hasattr(trainer.model.generator, \"print_trainable_parameters\"):\n",
    "            print(\"Trainable parameters for PEFT-enabled generator:\")\n",
    "            trainer.model.generator.print_trainable_parameters()\n",
    "        else:\n",
    "            print(\"Trainer model does not have 'print_trainable_parameters' method on its generator.\")\n",
    "\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    # Ensure RAG model forward pass works before training\n",
    "    print(\"Test forward pass prior to training\")\n",
    "    example_query_for_test = \"What did Socrates say in his trial?\"\n",
    "    example_answer_for_test = \"Socrates said wise things.\"\n",
    "    question_tok = question_encoder_tokenizer(example_query_for_test, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=128)\n",
    "    label_tok = generator_tokenizer(text_target=example_answer_for_test, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=64)\n",
    "\n",
    "    test_input = {\n",
    "        \"input_ids\": question_tok[\"input_ids\"].to('cuda'),\n",
    "        \"attention_mask\": question_tok[\"attention_mask\"].to('cuda'),\n",
    "        \"labels\": label_tok[\"input_ids\"].to('cuda')\n",
    "    }\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**test_input)\n",
    "        print(\"Test forward pass successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Starting RAG model training...\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"RAG model training completed.\")\n",
    "\n",
    "\n",
    "    # --- Save the Fine-tuned RAG Model ---\n",
    "    final_save_path = os.path.join(training_args.output_dir, \"final_model_for_inference\")\n",
    "\n",
    "    print(f\"Saving main model weights and RagConfig to: {final_save_path}\")\n",
    "    trainer.save_model(final_save_path)\n",
    "\n",
    "    if trainer.args.process_index == 0:\n",
    "        print(\"Manually saving component models and tokenizers to subdirectories...\")\n",
    "\n",
    "        # Unwrap the model to get access to its components\n",
    "        unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "\n",
    "        # Save the question encoder to its subdirectory\n",
    "        qe_path = os.path.join(final_save_path, \"question_encoder\")\n",
    "        unwrapped_model.rag.question_encoder.save_pretrained(qe_path)\n",
    "        print(f\"Question encoder component saved to {qe_path}\")\n",
    "\n",
    "        # Save the generator to its subdirectory\n",
    "        gen_path = os.path.join(final_save_path, \"generator\")\n",
    "        unwrapped_model.rag.generator.save_pretrained(gen_path)\n",
    "        print(f\"Generator component saved to {gen_path}\")\n",
    "\n",
    "        # Save the RagTokenizer\n",
    "        rag_tokenizer_to_save = RagTokenizer(\n",
    "            question_encoder=question_encoder_tokenizer,\n",
    "            generator=generator_tokenizer\n",
    "        )\n",
    "        rag_tokenizer_to_save.save_pretrained(final_save_path)\n",
    "        print(f\"RagTokenizer components saved to subfolders in {final_save_path}\")\n",
    "\n",
    "    # Wait for all processes to finish before exiting\n",
    "    if trainer.is_fsdp_enabled or trainer.args.world_size > 1:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    print(f\"\\nTraining and saving complete. Your final, inference-ready model is at: {final_save_path}\")"
   ],
   "id": "a98607571bcdf69b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Client\n",
    "\n",
    "Configure the SDK client by providing the authentication token:"
   ],
   "id": "76e8441a03ab5493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kubernetes import client\n",
    "from kubeflow.training import TrainingClient\n",
    "from kubeflow.training.models import V1Volume, V1VolumeMount, V1PersistentVolumeClaimVolumeSource\n",
    "\n",
    "api_server = \"\"\n",
    "token = \"\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "api_client = client.ApiClient(configuration)\n",
    "client = TrainingClient(client_configuration=api_client.configuration)"
   ],
   "id": "2302b76e51f1e87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Job\n",
    "You're now almost ready to create the training job:\n",
    "* Fill the `HF_HOME` environment variable with your HuggingFace token if you fine-tune a gated model\n",
    "* Check the number of worker nodes\n",
    "* Amend the resources per worker according to the job requirements\n",
    "* Update the PVC name to the one you've attached to the workbench if needed"
   ],
   "id": "b73ac5870ed16672"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "client.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=\"sft-rag\",\n",
    "    train_func=main,\n",
    "    num_workers=3,\n",
    "    num_procs_per_worker=\"1\",\n",
    "    resources_per_worker={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"memory\": \"64Gi\",\n",
    "        \"cpu\": 8,\n",
    "    },\n",
    "    base_image=\"quay.io/modh/training:py311-cuda124-torch251\",\n",
    "    env_vars={\n",
    "        \"CUDA_LAUNCH_BLOCKING\": \"1\",\n",
    "        \"HF_HOME\": \"/mnt/shared/.cache\",\n",
    "        \"HF_TOKEN\": \"\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"PYTORCH_HIP_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "    },\n",
    "    parameters=parameters,\n",
    "    volumes=[\n",
    "        V1Volume(name=\"shared\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"shared\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"shared\", mount_path=\"/mnt/shared\"),\n",
    "    ],\n",
    ")"
   ],
   "id": "1fc6a4357fdefc24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once the training job is created, you can follow its progress:",
   "id": "265f5235703bf4df"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "client.get_job_logs(\n",
    "    name=\"sft-rag\",\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    follow=True,\n",
    ")"
   ],
   "id": "d3a05a9a43b71c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TensorBoard\n",
    "\n",
    "You can track your job runs and visualize the training metrics with TensorBoard:"
   ],
   "id": "aaa5e08aed4ccfef"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import os\nos.environ[\"TENSORBOARD_PROXY_URL\"]= os.environ[\"NB_PREFIX\"]+\"/proxy/6006/\"",
   "id": "911c92f9690c974d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "%load_ext tensorboard",
   "id": "8f4ba65938f4c2c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "%tensorboard --logdir /opt/app-root/src/shared",
   "id": "631b14f745f11343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing the Fine-Tuned RAG Model\n",
    "After fine-tuning, you can load the entire RAG model and test its performance by providing a question.\n",
    "The `RagModel` will internally perform retrieval and then generate an answer based on the retrieved context."
   ],
   "id": "f9685e2995c22d8"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Install / upgrade dependencies\n!pip install --upgrade transformers peft accelerate sentence-transformers",
   "id": "277d7ce3e267f46d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    RagSequenceForGeneration,\n",
    "    RagConfig, AutoModelForQuestionAnswering, AutoModel\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# This is required for `feast_rag_retriever` to be found (This is temporary until Feast RAG Retriever is part of Feast SDK)\n",
    "CUSTOM_MODULES_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto_feast_rag\"\n",
    "sys.path.append(CUSTOM_MODULES_PATH)\n",
    "from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "\n",
    "STORE_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag/feature_repo\"\n",
    "FINETUNED_RAG_CHECKPOINT_DIR = \"/opt/app-root/src/shared/fine_tuned_rag_model/final_model_for_inference\"\n",
    "\n",
    "# --- 1. Initialize Custom Retriever ---\n",
    "print(\"Initializing custom FeastRAGRetriever...\")\n",
    "\n",
    "rag_config_inference = RagConfig.from_pretrained(FINETUNED_RAG_CHECKPOINT_DIR)\n",
    "question_encoder_tokenizer_inference = AutoTokenizer.from_pretrained(os.path.join(FINETUNED_RAG_CHECKPOINT_DIR, 'question_encoder_tokenizer'))\n",
    "generator_tokenizer_inference = AutoTokenizer.from_pretrained(os.path.join(FINETUNED_RAG_CHECKPOINT_DIR, 'generator_tokenizer'))\n",
    "\n",
    "if generator_tokenizer_inference.pad_token is None:\n",
    "    generator_tokenizer_inference.pad_token = generator_tokenizer_inference.eos_token\n",
    "\n",
    "feast_index_inference = FeastIndex()\n",
    "query_encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "features_to_retrieve = [\n",
    "    \"wiki_passages:passage_text\",\n",
    "    \"wiki_passages:embedding\",\n",
    "    \"wiki_passages:passage_id\",\n",
    "]\n",
    "\n",
    "# Initialize FeastRAGRetriever with the loaded tokenizers and config\n",
    "rag_retriever_inference = FeastRAGRetriever(\n",
    "    question_encoder_tokenizer=question_encoder_tokenizer_inference,\n",
    "    generator_tokenizer=generator_tokenizer_inference,\n",
    "    question_encoder=None,\n",
    "    generator_model=None,\n",
    "    feast_repo_path=STORE_PATH,\n",
    "    feature_view=wiki_passage_feature_view,\n",
    "    features=features_to_retrieve,\n",
    "    search_type=\"vector\",\n",
    "    config=rag_config_inference,\n",
    "    index=feast_index_inference,\n",
    "    query_encoder_model=query_encoder_model,\n",
    ")\n",
    "print(\"FeastRAGRetriever initialized successfully.\")\n",
    "\n",
    "\n",
    "# --- 2. Load Fine-Tuned RAG Model ---\n",
    "print(f\"Loading full RagSequenceForGeneration model from: {FINETUNED_RAG_CHECKPOINT_DIR}\")\n",
    "\n",
    "try:\n",
    "    finetuned_rag_model = RagSequenceForGeneration.from_pretrained(\n",
    "        FINETUNED_RAG_CHECKPOINT_DIR,\n",
    "        retriever=rag_retriever_inference\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    finetuned_rag_model.to(device)\n",
    "    finetuned_rag_model.eval()\n",
    "    print(f\"Fine-tuned RAG model loaded and moved to {device}. Ready for inference.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load RagSequenceForGeneration model. Ensure the directory '{FINETUNED_RAG_CHECKPOINT_DIR}' contains the correct RAG model structure (config.json and subfolders for generator, question_encoder, etc.).\")\n",
    "    raise e\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "\n",
    "test_queries = [\n",
    "    \"Which formations have a lower diversity of documented dinosaurs?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"When was the Declaration of Independence signed?\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    print(f\"\\nQuery: {test_query}\")\n",
    "    try:\n",
    "        # Tokenize the query using the question encoder's tokenizer\n",
    "        inputs = question_encoder_tokenizer_inference(\n",
    "            test_query,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate the answer\n",
    "        generated_ids = finetuned_rag_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=5,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "\n",
    "        # Decode the answer using the generator's tokenizer\n",
    "        answer = generator_tokenizer_inference.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Answer: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for query '{test_query}': {e}\")\n",
    "        raise\n"
   ],
   "id": "4346493cb61f36f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleaning Up",
   "id": "566467af21514aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Job\n",
    "\n",
    "Once you're done or want to re-create the training job, you can delete the existing one:"
   ],
   "id": "b9fdce6d97adac68"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "client.delete_job(name=\"sft-rag\")",
   "id": "246dc3833236929d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GPU Memory\n",
    "If you want to start over and test the pre-trained model again, you can free the GPU / accelerator memory with:"
   ],
   "id": "bbd537f372b6e385"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "\n",
    "del finetuned_rag_model, rag_retriever_inference, rag_config_inference\n",
    "del question_encoder_tokenizer_inference, generator_tokenizer_inference, feast_index_inference, query_encoder_model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "83c74f71847d1807",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

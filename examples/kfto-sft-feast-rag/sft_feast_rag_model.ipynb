{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da333c8bd896040",
   "metadata": {},
   "source": "# Setup"
  },
  {
   "cell_type": "code",
   "id": "5fdbc4d625275778",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# Install the YAML magic\n",
    "%pip install yamlmagic\n",
    "%load_ext yamlmagic"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27b26c61436737ee",
   "metadata": {},
   "source": "# Training Configuration\nEdit the following training hyperparameters and can be modified to experiment with learning rates, batch\\ sizes, LoRA parameters:"
  },
  {
   "cell_type": "code",
   "id": "e9d2cab9c84e4443",
   "metadata": {
    "trusted": false
   },
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Model\n",
    "model_name_or_path: facebook/bart-large    # only works with Ses2Seq (Encoder-Decoder) models like BART and T5 since transformers RAG only support them for now.\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: eager                # one of eager (default), sdpa or flash_attention_2\n",
    "use_liger: false                          # use Liger kernels\n",
    "\n",
    "# PEFT / LoRA (Apply to Generator Model)\n",
    "use_peft: false\n",
    "lora_r: 16\n",
    "lora_alpha: 8\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]   # Ensure these match your generator model\n",
    "lora_modules_to_save: []\n",
    "\n",
    "# QLoRA (BitsAndBytes) (Apply to Generator Model)\n",
    "load_in_4bit: false                       # use 4 bit precision for the base model (only with LoRA)\n",
    "load_in_8bit: false                       # use 8 bit precision for the base model (only with LoRA)\n",
    "\n",
    "# Dataset\n",
    "dataset_name: facebook/wiki_dpr\n",
    "dataset_config: main                      # name of the dataset configuration\n",
    "dataset_train_split: train                # dataset split to use for training (for RAG generated data)\n",
    "dataset_test_split: test                  # dataset split to use for evaluation (for RAG generated data)\n",
    "dataset_kwargs:\n",
    "    add_special_tokens: false               # template with special tokens\n",
    "    append_concat_token: false              # add additional separator token\n",
    "\n",
    "# SFT (These parameters will now apply to the RagModel's training)\n",
    "max_seq_length: 1024                      # max sequence length for model and packing of the dataset\n",
    "dataset_batch_size: 1000                  # samples to tokenize per batch (for initial data processing)\n",
    "packing: false                            # Packing is generally not used directly with RagModel training in the same way as SFT\n",
    "\n",
    "# Training\n",
    "num_train_epochs: 3                       # number of training epochs\n",
    "remove_unused_columns: false\n",
    "label_smoothing_factor: 0.1                # 0.1, 0.0(disable)\n",
    "\n",
    "per_device_train_batch_size: 1            # Batch size per device during training\n",
    "per_device_eval_batch_size: 1             # Batch size for evaluation\n",
    "auto_find_batch_size: false               # find a batch size that fits into memory automatically\n",
    "eval_strategy: epoch                      # evaluate every epoch\n",
    "\n",
    "bf16: true                                # use bf16 16-bit (mixed) precision\n",
    "tf32: true                               # use tf32 precision\n",
    "\n",
    "learning_rate: 4.0e-6                     # 4.0e-6 Initial learning rate for RAG model training\n",
    "warmup_steps: 200                         # steps for a linear warmup from 0 to `learning_rate`\n",
    "lr_scheduler_type: cosine                 # learning rate scheduler (see transformers.SchedulerType)\n",
    "\n",
    "optim: adamw_torch_fused                  # optimizer (see transformers.OptimizerNames)\n",
    "max_grad_norm: 1.0                        # max gradient norm\n",
    "seed: 42\n",
    "\n",
    "gradient_accumulation_steps: 8            # Increase for smaller per_device_train_batch_size\n",
    "gradient_checkpointing: false             # use gradient checkpointing to save memory\n",
    "gradient_checkpointing_kwargs:\n",
    "    use_reentrant: false\n",
    "\n",
    "# FSDP\n",
    "fsdp: \"full_shard auto_wrap\"              # add offload if not enough GPU memory\n",
    "fsdp_config:\n",
    "    activation_checkpointing: true\n",
    "    cpu_ram_efficient_loading: false\n",
    "    sync_module_states: true\n",
    "    use_orig_params: true\n",
    "    limit_all_gathers: false\n",
    "\n",
    "\n",
    "# fsdp_transformer_layer_cls_to_wrap: [BertLayer, BartEncoderLayer, BartDecoderLayer]\n",
    "\n",
    "# Checkpointing\n",
    "save_strategy: epoch                      # save checkpoint every epoch\n",
    "save_total_limit: 1                       # limit the total amount of checkpoints\n",
    "resume_from_checkpoint: true             # load the last checkpoint in output_dir and resume from it\n",
    "\n",
    "# Logging\n",
    "log_level: warning                        # logging level (see transformers.logging)\n",
    "logging_strategy: steps\n",
    "logging_steps: 1                          # log every N steps\n",
    "report_to:\n",
    "- tensorboard                             # report metrics to tensorboard\n",
    "\n",
    "output_dir: /mnt/shared/fine_tuned_rag_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eaf49344963140de",
   "metadata": {},
   "source": "# Feast Setup with Milvus\n"
  },
  {
   "cell_type": "markdown",
   "id": "5f4f28a4-1c18-488a-ad74-a8c857ebd0f1",
   "metadata": {},
   "source": "### Install Required Dependencies "
  },
  {
   "cell_type": "code",
   "id": "47aac78970e46f74",
   "metadata": {
    "trusted": false
   },
   "source": [
    "%%bash\n",
    "pip install --quiet feast[milvus] sentence-transformers datasets\n",
    "pip install bigtree==0.19.2\n",
    "pip install marshmallow==3.10.0\n",
    "pip install feast"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2e42b594-0ada-4ef8-8922-daed60745d2b",
   "metadata": {},
   "source": "## Loading Wikipedia Dataset\nWe only load a subset of the dataset in the interest of keeping this example runnable with minimum memory and storage."
  },
  {
   "cell_type": "code",
   "id": "468e72e9-8410-4775-961d-0ec1501e74f0",
   "metadata": {
    "trusted": false
   },
   "source": [
    "from datasets import load_dataset\n",
    "# load wikipedia dataset - 5% of the training split\n",
    "dataset = load_dataset(\n",
    "    \"facebook/wiki_dpr\",\n",
    "    \"psgs_w100.nq.exact\",\n",
    "    split=\"train[:5%]\",\n",
    "    with_index=False,\n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "876e5640-e71d-45d4-8acf-026209b4309b",
   "metadata": {},
   "source": "## Chunking Wikipedia Dataset\nThe dataset is chunked to contain a preset number of chars, which is the max supported by Feast. Ensuring the chunk only contains whole words, thus the retrieved context can form sentences without incomplete words."
  },
  {
   "cell_type": "code",
   "id": "815c9ec70c155da4",
   "metadata": {
    "trusted": false
   },
   "source": "def chunk_dataset(examples, max_chars=380):\n    all_chunks = []\n    all_ids = []\n    all_titles = []\n\n    for i, text in enumerate(examples['text']): # Iterate over texts in the batch\n        words = text.split()\n        if not words:\n            continue\n\n        current_chunk_words = []\n        for word in words:\n            # Check if adding the next word exceeds the character limit\n            if len(' '.join(current_chunk_words + [word])) > max_chars:\n                # If the current chunk is valid, save it\n                if current_chunk_words:\n                    chunk_text = ' '.join(current_chunk_words)\n                    all_chunks.append(chunk_text)\n                    all_ids.append(f\"{examples['id'][i]}_{len(all_chunks)}\") # Unique ID for the chunk\n                    all_titles.append(examples['title'][i])\n                # Start a new chunk with the current word\n                current_chunk_words = [word]\n            else:\n                current_chunk_words.append(word)\n\n        # Add the last remaining chunk\n        if current_chunk_words:\n            chunk_text = ' '.join(current_chunk_words)\n            all_chunks.append(chunk_text)\n            all_ids.append(f\"{examples['id'][i]}_{len(all_chunks)}\") # Unique ID for the chunk\n            all_titles.append(examples['title'][i])\n\n    return {'id': all_ids, 'title': all_titles, 'text': all_chunks}\n\n\nchunked_dataset = dataset.map(\n    chunk_dataset,\n    batched=True,\n    remove_columns=dataset.column_names,\n    num_proc=1\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff6b7551-4950-4eb4-aa5b-7d658a0effc7",
   "metadata": {},
   "source": "## Create DPR Embeddings\nWe load a pre-trained Dense Passage Retrieval (DPR) encoder to generate context embeddings for each chunked passage. These embeddings will later be stored in the Feast feature store and queried during retrieval."
  },
  {
   "cell_type": "code",
   "id": "40dc319fd9aa1ad4",
   "metadata": {
    "trusted": false
   },
   "source": [
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load DPR Context Encoder model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(embedding_model_name)\n",
    "model = DPRContextEncoder.from_pretrained(embedding_model_name).to(device)\n",
    "\n",
    "sentences = chunked_dataset[\"text\"]\n",
    "\n",
    "print(f\"Generating DPR embeddings for {len(sentences)} documents...\")\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(sentences), 16)): # Process in batches\n",
    "        batch_texts = sentences[i:i+16]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        all_embeddings.append(embeddings.to(dtype=torch.float32).cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Embeddings generated with shape: {embeddings.shape}\")\n",
    "print(f\"Saving generated embeddings and chunked sentences to file...\")\n",
    "np.save(\"/opt/app-root/src/shared/synthetic_data_cache/embed_data/embeddings.npy\", embeddings)\n",
    "with open(\"/opt/app-root/src/shared/synthetic_data_cache/embed_data/sentences.txt\", \"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(f\"{sentence}\\n\")\n",
    "\n",
    "print(\"saved\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "754d3715-0088-4439-8d5b-deb477ecb8c8",
   "metadata": {},
   "source": "## Create Parquet File for Feast Offline Store\nCreate a parquet file using the DPR embeddings created previously."
  },
  {
   "cell_type": "code",
   "id": "824d4b0b35f7eee6",
   "metadata": {
    "trusted": false
   },
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "embeddings = np.load(\"/opt/app-root/src/shared/synthetic_data_cache/embed_data/embeddings.npy\")\n",
    "with open(\"/opt/app-root/src/shared/synthetic_data_cache/embed_data/sentences.txt\", \"r\") as f:\n",
    "    sentences = [line.strip() for line in f]\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Prepare first batch to initialize schema\n",
    "first_batch_sentences = sentences[:batch_size]\n",
    "first_batch_embeddings = embeddings[:batch_size]\n",
    "\n",
    "first_batch_df = pd.DataFrame({\n",
    "    \"passage_id\": list(range(batch_size)),\n",
    "    \"passage_text\": first_batch_sentences,\n",
    "    \"embedding\": pd.Series([embedding.tolist() for embedding in first_batch_embeddings], dtype=object),\n",
    "    \"event_timestamp\": [datetime.now(timezone.utc)] * len(first_batch_sentences)\n",
    "})\n",
    "\n",
    "print(\"DataFrame Info:\")\n",
    "print(first_batch_df.head())\n",
    "print(first_batch_df[\"embedding\"].apply(lambda x: len(x) if isinstance(x, list) else str(type(x))).value_counts())\n",
    "\n",
    "# Initialize Parquet writer with correct schema\n",
    "pqwriter = pq.ParquetWriter('feature_repo/wiki_dpr_1perct.parquet', pa.Table.from_pandas(first_batch_df).schema)\n",
    "\n",
    "# Write first batch\n",
    "pqwriter.write_table(pa.Table.from_pandas(first_batch_df))\n",
    "\n",
    "# Continue writing remaining batches\n",
    "for i in range(batch_size, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    batch_embeddings = embeddings[i:i+batch_size]\n",
    "\n",
    "    batch_df = pd.DataFrame({\n",
    "        \"passage_id\": list(range(i, i + len(batch_sentences))),\n",
    "        \"passage_text\": batch_sentences,\n",
    "        \"embedding\": pd.Series([embedding.tolist() for embedding in batch_embeddings]),\n",
    "        \"event_timestamp\": [datetime.now(timezone.utc)] * len(batch_sentences)\n",
    "    })\n",
    "\n",
    "    pqwriter.write_table(pa.Table.from_pandas(batch_df))\n",
    "    print(f\"Wrote {i + len(batch_sentences)} / {len(sentences)} documents...\")\n",
    "\n",
    "pqwriter.close()\n",
    "print(\"Saved to wiki_dpr.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "378f8b2c2c50ecfb",
   "metadata": {
    "trusted": false
   },
   "source": "%cd feature_repo",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Feast Feature Repository",
   "id": "b7a775e6b6fac1d6"
  },
  {
   "cell_type": "code",
   "id": "1a9d6a3af2085f6e",
   "metadata": {
    "trusted": false
   },
   "source": "!feast apply",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d35cb27-fe51-4f1b-ab48-fb41c0fcb7d7",
   "metadata": {},
   "source": "## Writing to Feast Online Store (Milvus)\nWe load the Parquet file into Milvus via Feast, which will serve as the online store for efficient similarity search during retrieval."
  },
  {
   "cell_type": "code",
   "id": "e88f8fc7be3137bb",
   "metadata": {
    "trusted": false
   },
   "source": [
    "import pyarrow.parquet as pq\n",
    "from feast import FeatureStore\n",
    "from pymilvus import MilvusException\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "parquet_file = pq.ParquetFile(\"./wiki_dpr.parquet\")\n",
    "batch_size = 10000\n",
    "\n",
    "for batch_num, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), 1):\n",
    "    batch_df = batch.to_pandas()\n",
    "    try:\n",
    "        print(f\"Writing batch {batch_num}...\")\n",
    "        store.write_to_online_store(feature_view_name='wiki_passages', df=batch_df)\n",
    "        print(f\"Batch {batch_num} written successfully.\")\n",
    "    except MilvusException as e:\n",
    "        print(f\"Skipping write of batch {batch_num} due to : {e}\")\n",
    "\n",
    "print(\"All data written to online store.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf154268-846e-4d9c-8586-7d260dc30106",
   "metadata": {},
   "source": "# Preprocessing Natural Questions Dataset for RAG Training\nWe prepare the Natural Questions dataset for RAG model training. The Dataset contains Questions and Answers. The dataset is quite big, and again in the interest of keepig this example runnable, we are using a subset of training & validation samples. "
  },
  {
   "cell_type": "code",
   "id": "90d69b21221a99f4",
   "metadata": {
    "trusted": false
   },
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DPRQuestionEncoderTokenizer,\n",
    "    DPRQuestionEncoder,\n",
    "    RagConfig\n",
    ")\n",
    "\n",
    "print(\"Preparing training data (Q&A pairs) with caching...\")\n",
    "processed_data_cache_dir = \"dataset/rag_3k_bart_intersection_dataset\"\n",
    "Path(processed_data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load question encoder tokenizer and model\n",
    "question_encoder_model_name_or_path = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_encoder_model_name_or_path)\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    question_encoder_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Load generator tokenizer\n",
    "generator_model_name_or_path = \"facebook/bart-large\"\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    generator_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    generator_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "import sys\n",
    "# Initialize FeastRAGRetriever for filtering answerable questions\n",
    "CUSTOM_MODULES_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag\"\n",
    "sys.path.append(CUSTOM_MODULES_PATH)\n",
    "\n",
    "# Import custom retriever and feature view definitions\n",
    "from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "# Define paths for Feast store and fine-tuned model checkpoint\n",
    "STORE_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag/feature_repo\"\n",
    "\n",
    "# Initialize components for question filtering\n",
    "store_path = \"feature_repo\"\n",
    "\n",
    "# Initialize FeastRAGRetriever\n",
    "feast_index = FeastIndex()\n",
    "rag_top_k = 10\n",
    "\n",
    "question_encoder_config = {\n",
    "    \"model_type\": \"dpr\",\n",
    "    \"hidden_size\": 768,\n",
    "    \"vocab_size\": question_encoder_tokenizer.vocab_size,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"projection_dim\": 0,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "}\n",
    "\n",
    "rag_config = RagConfig(\n",
    "    question_encoder=question_encoder_config,\n",
    "    generator=generator_model.config.to_dict(),\n",
    "    index_name=\"custom\",\n",
    "    index={\"index_name\": \"feast_dummy_index\", \"custom_type\": \"FeastIndex\"},\n",
    "    n_docs=rag_top_k,\n",
    ")\n",
    "\n",
    "features_to_retrieve = [\n",
    "    \"wiki_passages:passage_text\",\n",
    "    \"wiki_passages:embedding\",\n",
    "    \"wiki_passages:passage_id\",\n",
    "]\n",
    "\n",
    "# Create RAG retriever for filtering\n",
    "rag_retriever = FeastRAGRetriever(\n",
    "    question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "    generator_tokenizer=generator_tokenizer,\n",
    "    question_encoder=question_encoder,\n",
    "    generator_model=generator_model,\n",
    "    feast_repo_path=STORE_PATH,\n",
    "    feature_view=wiki_passage_feature_view,\n",
    "    features=features_to_retrieve,\n",
    "    search_type=\"vector\",\n",
    "    config=rag_config,\n",
    "    index=feast_index,\n",
    ")\n",
    "\n",
    "print(\"FeastRAGRetriever initialized for filtering.\")\n",
    "\n",
    "# Filter function to retain only examples with valid short answers\n",
    "def has_valid_answer(answer):\n",
    "    question_text = example[\"question\"][\"text\"]\n",
    "    if not answer[\"annotations\"][\"short_answers\"]:\n",
    "        return False\n",
    "\n",
    "    for short_ans_dict in answer[\"annotations\"][\"short_answers\"]:\n",
    "        if \"text\" in short_ans_dict and short_ans_dict[\"text\"] and short_ans_dict[\"text\"][0].strip():\n",
    "\n",
    "            expected_answer = short_ans_dict[\"text\"][0]\n",
    "            return rag_retriever.is_question_answerable(\n",
    "                question_text=question_text,\n",
    "                expected_answer=expected_answer,\n",
    "                question_encoder=question_encoder,\n",
    "                tokenizer=question_encoder_tokenizer,\n",
    "                top_k=10,\n",
    "                similarity_threshold=0.6,\n",
    "                max_answer_length=50,\n",
    "                min_answer_length=1\n",
    "            )\n",
    "    return False\n",
    "\n",
    "# Preprocessing function to tokenize question and answer for RAG input\n",
    "def preprocess_nq_example(example):\n",
    "    question_text = example[\"question\"][\"text\"]\n",
    "    answer_text = \"\"\n",
    "\n",
    "    # Select the first available short answer\n",
    "    if example[\"annotations\"][\"short_answers\"]:\n",
    "        for short_ans_dict in example[\"annotations\"][\"short_answers\"]:\n",
    "            if \"text\" in short_ans_dict and short_ans_dict[\"text\"] and short_ans_dict[\"text\"][0].strip():\n",
    "                answer_text = short_ans_dict[\"text\"][0]\n",
    "\n",
    "    # Tokenize question for the RAG model's question encoder\n",
    "    tokenized_question = question_encoder_tokenizer(\n",
    "        question_text,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize answer for the RAG model's generator\n",
    "    tokenized_answer_for_labels = generator_tokenizer(\n",
    "        text_target=answer_text,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_question[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_question[\"attention_mask\"],\n",
    "        \"labels\": tokenized_answer_for_labels[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# nq_dataset_name = \"natural_questions\"\n",
    "nq_dataset_name = \"google-research-datasets/natural_questions\"\n",
    "nq_train_split = \"train\"\n",
    "nq_test_split = \"test\"\n",
    "\n",
    "# Check if preprocessed datasets are cached\n",
    "if (Path(processed_data_cache_dir) / nq_train_split).exists() and \\\n",
    "        (Path(processed_data_cache_dir) / nq_test_split).exists():\n",
    "    print(f\"Loading preprocessed data from cache: {processed_data_cache_dir}\")\n",
    "    loaded_processed_datasets = load_from_disk(processed_data_cache_dir)\n",
    "    train_dataset = loaded_processed_datasets[nq_train_split]\n",
    "    test_dataset = loaded_processed_datasets[nq_test_split]\n",
    "\n",
    "else:\n",
    "    print(\"Loading raw Natural Questions dataset (streaming) and preprocessing...\")\n",
    "\n",
    "    num_train_samples = 3000  # Target number of valid training samples\n",
    "    num_eval_samples = 300    # Target number of valid evaluation samples\n",
    "\n",
    "    raw_train_stream = load_dataset(nq_dataset_name, \"default\", split=nq_train_split, streaming=True)\n",
    "    raw_eval_stream = load_dataset(nq_dataset_name, \"default\", split=\"validation\", streaming=True)\n",
    "\n",
    "    # Stream and filter training data on the fly until target count is reached\n",
    "    temp_raw_train_list = []\n",
    "\n",
    "    for example in raw_train_stream:\n",
    "        if has_valid_answer(example):\n",
    "            temp_raw_train_list.append(example)\n",
    "            if len(temp_raw_train_list) >= num_train_samples:\n",
    "                break\n",
    "\n",
    "    # Stream and filter evaluation data on the fly until target count is reached\n",
    "    temp_raw_eval_list = []\n",
    "    for example in raw_eval_stream:\n",
    "        if has_valid_answer(example):\n",
    "            temp_raw_eval_list.append(example)\n",
    "            if len(temp_raw_eval_list) >= num_eval_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "    raw_datasets_dict = DatasetDict({\n",
    "        nq_train_split: Dataset.from_list(temp_raw_train_list),\n",
    "        nq_test_split: Dataset.from_list(temp_raw_eval_list)\n",
    "    })\n",
    "\n",
    "    print(\"Applying preprocessing to filtered samples...\")\n",
    "    processed_datasets = raw_datasets_dict.map(\n",
    "        preprocess_nq_example,\n",
    "        remove_columns=raw_datasets_dict[nq_train_split].column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[nq_train_split]\n",
    "    test_dataset = processed_datasets[nq_test_split]\n",
    "\n",
    "    print(f\"Saving preprocessed datasets to cache: {processed_data_cache_dir}\")\n",
    "    processed_datasets.save_to_disk(processed_data_cache_dir)\n",
    "    print(\"Preprocessed data saved to cache.\")\n",
    "\n",
    "# Dataset summary\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train dataset columns: {train_dataset.column_names}\")\n",
    "print(f\"Sample from train dataset: {train_dataset[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8879b1d8-5086-449a-8872-6e79d705d97a",
   "metadata": {},
   "source": "### Dataset Sanity Check\nRandomly inspect samples from the preprocessed dataset to ensure questions and answers are correctly tokenized and properly structured."
  },
  {
   "cell_type": "code",
   "id": "afb491790cde0e7e",
   "metadata": {
    "trusted": false
   },
   "source": "import random\n\nfor index in random.sample(range(len(train_dataset)), min(3, len(train_dataset))):\n    print(f\"\\n  Processed Sample {index}  \")\n    print(f\"Question: {train_dataset[index].keys()}\")\n    print(f\"Decoded Question: {question_encoder_tokenizer.decode(train_dataset[index]['input_ids'], skip_special_tokens=True)}\")\n    print(f\"Decoded Labels (Answer): {generator_tokenizer.decode(train_dataset[index]['labels'], skip_special_tokens=True)}\")\n    print(f\"Raw Input IDs (Question): {train_dataset[index]['input_ids'][:20]}...\")\n    print(f\"Raw Labels (Answer): {train_dataset[index]['labels'][:20]}...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4473b366d7156b5",
   "metadata": {},
   "source": "### Preparing Training Assets for Distributed Training\n\nCopy over the `kfto-sft-feast-rag` folder into the shared storage, so that it can be accessed by the training job, since the FeastRagRetriever requires the `feature_repo` directory during training and inference. Copy over the `feast_rag_retriever.py` script as well, so that it can be imported in the training job."
  },
  {
   "cell_type": "code",
   "id": "d48069de28ffe69",
   "metadata": {
    "trusted": false
   },
   "source": "%cp -r $HOME/distributed-workloads/examples/kfto-sft-feast-rag $HOME/shared/",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4c88f6d4a5099b1",
   "metadata": {},
   "source": "# Fine-tuning the RAG Model (Training Loop)\nWe initiate the fine-tuning process for the RAG model using the Seq2SeqTrainer. This step integrates the retriever, encoder, generator, and custom datasets to jointly optimize the generation task."
  },
  {
   "cell_type": "code",
   "id": "9f97070e5ea5bae8",
   "metadata": {
    "trusted": false
   },
   "source": [
    "def main(parameters):\n",
    "    import subprocess, sys\n",
    "    # Install necessary packages\n",
    "    # This ensures Feast and other dependencies are available inside the container\n",
    "    print(\"Installing Feast and other RAG dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"feast[milvus]\", \"sentence-transformers\", \"datasets\", \"bigtree==0.19.2\", \"marshmallow==3.10.0\", \"protobuf>=3.10.0\", \"git+https://github.com/feast-dev/feast.git@master\"])\n",
    "    print(\"Feast and other RAG dependencies installed.\")\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        set_seed,\n",
    "        RagSequenceForGeneration,\n",
    "        GenerationConfig,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        Seq2SeqTrainer,\n",
    "        RagTokenizer,\n",
    "        default_data_collator,\n",
    "        RagConfig,\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        DPRQuestionEncoderTokenizer,\n",
    "        DPRQuestionEncoder\n",
    "    )\n",
    "    from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "    import torch\n",
    "\n",
    "    from trl import (\n",
    "        ModelConfig,\n",
    "        ScriptArguments,\n",
    "        SFTConfig,\n",
    "        TrlParser,\n",
    "        get_peft_config,\n",
    "    )\n",
    "\n",
    "    # This is required for `feast_rag_retriever` to be found (for debugging purposes)\n",
    "    CUSTOM_MODULES_PATH = \"/mnt/shared/kfto-sft-feast-rag\"\n",
    "    sys.path.append(CUSTOM_MODULES_PATH)\n",
    "    print(f\"Added {CUSTOM_MODULES_PATH} to sys.path for custom module imports.\")\n",
    "    # from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "    from feast.rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "    from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args_trl, model_args = parser.parse_dict(parameters)\n",
    "\n",
    "    # Convert SFTConfig parameters to standard TrainingArguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=training_args_trl.output_dir,\n",
    "        num_train_epochs=training_args_trl.num_train_epochs,\n",
    "        per_device_train_batch_size=training_args_trl.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_args_trl.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=training_args_trl.gradient_accumulation_steps,\n",
    "        gradient_checkpointing=training_args_trl.gradient_checkpointing,\n",
    "        learning_rate=training_args_trl.learning_rate,\n",
    "        warmup_steps=training_args_trl.warmup_steps,\n",
    "        lr_scheduler_type=training_args_trl.lr_scheduler_type,\n",
    "        optim=training_args_trl.optim,\n",
    "        max_grad_norm=training_args_trl.max_grad_norm,\n",
    "        seed=training_args_trl.seed,\n",
    "        bf16=training_args_trl.bf16,\n",
    "        tf32=training_args_trl.tf32,\n",
    "        eval_strategy=training_args_trl.eval_strategy,\n",
    "        save_strategy=training_args_trl.save_strategy,\n",
    "        save_total_limit=training_args_trl.save_total_limit,\n",
    "        logging_strategy=training_args_trl.logging_strategy,\n",
    "        logging_steps=training_args_trl.logging_steps,\n",
    "        report_to=training_args_trl.report_to,\n",
    "        fsdp=training_args_trl.fsdp if hasattr(training_args_trl, 'fsdp') else None,\n",
    "        fsdp_config=training_args_trl.fsdp_config if hasattr(training_args_trl, 'fsdp_config') else None,\n",
    "        resume_from_checkpoint=training_args_trl.resume_from_checkpoint,\n",
    "        remove_unused_columns=training_args_trl.remove_unused_columns,\n",
    "        predict_with_generate= True,\n",
    "    )\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Question Encoder\n",
    "    question_encoder_model_name_or_path = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "    question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_encoder_model_name_or_path)\n",
    "    question_encoder_model = DPRQuestionEncoder.from_pretrained(\n",
    "        question_encoder_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        torch_dtype=model_args.torch_dtype\n",
    "    )\n",
    "\n",
    "    # Generator Model to be fine-tuned\n",
    "    generator_model_name_or_path = model_args.model_name_or_path\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        use_fast=True\n",
    "    )\n",
    "    generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=model_args.torch_dtype,\n",
    "        use_cache=not training_args.gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "    if model_args.use_peft:\n",
    "        print(f\"PEFT/LoRA: Ensure 'lora_target_modules' in your parameters ({parameters.get('lora_target_modules')}) are correct for the new generator model '{generator_model_name_or_path}'.\")\n",
    "        generator_model = prepare_model_for_kbit_training(generator_model)\n",
    "        peft_config = get_peft_config(model_args)\n",
    "        generator_model = get_peft_model(generator_model, peft_config)\n",
    "        print(\"PEFT setup for generator model completed.\")\n",
    "\n",
    "    store_path = CUSTOM_MODULES_PATH+\"/feature_repo\"\n",
    "\n",
    "    # Initialize FeastRAGRetriever\n",
    "    feast_index = FeastIndex()\n",
    "    rag_top_k = 10\n",
    "\n",
    "    question_encoder_config = {\n",
    "        \"model_type\": \"dpr\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"vocab_size\": question_encoder_tokenizer.vocab_size,\n",
    "        \"num_hidden_layers\": 6,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"projection_dim\": 0,\n",
    "        \"torch_dtype\": model_args.torch_dtype\n",
    "    }\n",
    "\n",
    "    rag_config = RagConfig(\n",
    "        question_encoder=question_encoder_config,\n",
    "        generator=generator_model.config.to_dict(),\n",
    "        index_name=\"custom\",\n",
    "        index={\"index_name\": \"feast_dummy_index\", \"custom_type\": \"FeastIndex\"},\n",
    "        n_docs=rag_top_k,\n",
    "    )\n",
    "\n",
    "    features_to_retrieve = [\n",
    "        \"wiki_passages:passage_text\",\n",
    "        \"wiki_passages:embedding\",\n",
    "        \"wiki_passages:passage_id\",\n",
    "    ]\n",
    "\n",
    "    rag_retriever = FeastRAGRetriever(\n",
    "        question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "        generator_tokenizer=generator_tokenizer,\n",
    "        feast_repo_path=store_path,\n",
    "        feature_view=wiki_passage_feature_view,\n",
    "        features=features_to_retrieve,\n",
    "        search_type=\"vector\",\n",
    "        config=rag_config,\n",
    "        index=feast_index,\n",
    "    )\n",
    "\n",
    "    # Initialize the RagModel for fine-tuning\n",
    "    model = RagSequenceForGeneration(\n",
    "        question_encoder=question_encoder_model,\n",
    "        config=rag_config,\n",
    "        generator=generator_model,   # model being fine-tuned\n",
    "        retriever=rag_retriever\n",
    "    )\n",
    "    generator_config = GenerationConfig(\n",
    "        max_length=128,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "    model.generation_config = generator_config\n",
    "    print(\"RAG Model initialized.\")\n",
    "\n",
    "    # Explicitly move model to GPU\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load preprocessed NQ dataset\n",
    "    print(\"Preparing training data (Q&A pairs) with caching...\")\n",
    "    processed_data_cache_dir = \"/mnt/shared/kfto-sft-feast-rag/dataset/rag_3k_bart_intersection_dataset\"\n",
    "    Path(processed_data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nq_train_split = \"train\"\n",
    "    nq_validation_split = \"test\"\n",
    "    # Check if the preprocessed dataset is already cached\n",
    "    if (Path(processed_data_cache_dir) / nq_train_split).exists() and \\\n",
    "            (Path(processed_data_cache_dir) / nq_validation_split).exists():\n",
    "        print(f\"Loading preprocessed data from cache: {processed_data_cache_dir}\")\n",
    "        loaded_processed_datasets = load_from_disk(processed_data_cache_dir)\n",
    "        train_dataset = loaded_processed_datasets[nq_train_split]\n",
    "        test_dataset = loaded_processed_datasets[nq_validation_split]\n",
    "    else:\n",
    "        print(\"ERROR DATASET NOT FOUND\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Training the RagModel\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=generator_tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        if hasattr(trainer.model.generator, \"print_trainable_parameters\"):\n",
    "            print(\"Trainable parameters for PEFT-enabled generator:\")\n",
    "            trainer.model.generator.print_trainable_parameters()\n",
    "        else:\n",
    "            print(\"Trainer model does not have 'print_trainable_parameters' method on its generator.\")\n",
    "\n",
    "    # RagSequenceForGeneration model trains encoder and generator jointly by default, uncomment below to freeze encoder model\n",
    "    # unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "    # unwrapped_model.rag.question_encoder.requires_grad_(False)\n",
    "    # for param in unwrapped_model.rag.question_encoder.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    # Ensure RAG model forward pass works before training\n",
    "    print(\"Test forward pass prior to training\")\n",
    "    example_query_for_test = \"What is the primary language spoken in Brazil?\"\n",
    "    example_answer_for_test = \"Portuguese\"\n",
    "    print(f\"Query: {example_query_for_test}\")\n",
    "    question_tok = question_encoder_tokenizer(example_query_for_test, return_tensors=\"pt\")\n",
    "    label_tok = generator_tokenizer(text_target=example_answer_for_test, return_tensors=\"pt\")\n",
    "\n",
    "    test_input = {\n",
    "        \"input_ids\": question_tok[\"input_ids\"].to('cuda'),\n",
    "        \"attention_mask\": question_tok[\"attention_mask\"].to('cuda'),\n",
    "        \"labels\": label_tok[\"input_ids\"].to('cuda')\n",
    "    }\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            model(**test_input)\n",
    "        print(\"Test forward pass successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Starting RAG model training...\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"RAG model training completed.\")\n",
    "\n",
    "\n",
    "    # Save the Fine-tuned RAG Model\n",
    "    final_save_path = os.path.join(training_args.output_dir, \"inference_\" + os.getenv(\"HOSTNAME\", \"rag_model\"))\n",
    "\n",
    "    print(f\"Saving main model weights and RagConfig to: {final_save_path}\")\n",
    "    trainer.save_model(final_save_path)\n",
    "\n",
    "    if trainer.args.process_index == 0:\n",
    "        print(\"Manually saving component models and tokenizers to subdirectories...\")\n",
    "\n",
    "        # Unwrap the model to get access to its components\n",
    "        unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "\n",
    "        # Save the question encoder to its subdirectory\n",
    "        qe_path = os.path.join(final_save_path, \"question_encoder\")\n",
    "        unwrapped_model.rag.question_encoder.save_pretrained(qe_path)\n",
    "        print(f\"Question encoder component saved to {qe_path}\")\n",
    "\n",
    "        # Save the generator to its subdirectory\n",
    "        gen_path = os.path.join(final_save_path, \"generator\")\n",
    "        unwrapped_model.rag.generator.save_pretrained(gen_path)\n",
    "        print(f\"Generator component saved to {gen_path}\")\n",
    "\n",
    "        # Save the RagTokenizer\n",
    "        rag_tokenizer_to_save = RagTokenizer(\n",
    "            question_encoder=question_encoder_tokenizer,\n",
    "            generator=generator_tokenizer\n",
    "        )\n",
    "        rag_tokenizer_to_save.save_pretrained(final_save_path)\n",
    "        print(f\"RagTokenizer components saved to subfolders in {final_save_path}\")\n",
    "\n",
    "    # Wait for all processes to finish before exiting\n",
    "    if trainer.is_fsdp_enabled or trainer.args.world_size > 1:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    print(f\"\\nTraining and saving complete. Your final, inference-ready model is at: {final_save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "285767a94abcd6a2",
   "metadata": {},
   "source": "# Kubeflow Training Client\nConfigure the Kubeflow SDK client by providing the required credentials to connect to a cluster."
  },
  {
   "cell_type": "code",
   "id": "8131fb67acb2267a",
   "metadata": {
    "trusted": false
   },
   "source": [
    "from kubernetes import client\n",
    "from kubeflow.training import TrainingClient\n",
    "from kubeflow.training.models import V1Volume, V1VolumeMount, V1PersistentVolumeClaimVolumeSource\n",
    "\n",
    "api_server = \"\"\n",
    "token = \"\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "api_client = client.ApiClient(configuration)\n",
    "client = TrainingClient(client_configuration=api_client.configuration)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6cd4e0ba3924317",
   "metadata": {},
   "source": [
    "# Training Job\n",
    "You're now almost ready to create the training job:\n",
    "* Fill the `HF_TOKEN` environment variable with your HuggingFace token if you fine-tune a gated model\n",
    "* Check the number of worker nodes\n",
    "* Amend the resources per worker according to the job requirements\n",
    "* If you use AMD accelerators:\n",
    "  * Change `nvidia.com/gpu` to `amd.com/gpu` in `resources_per_worker`\n",
    "  * Change `base_image` to `quay.io/modh/training:py311-rocm62-torch251`\n",
    "* Update the PVC name to the one you've attached to the workbench if needed\n",
    "* Fill in the queue name if Kueue is enabled in the cluster, otherwise remove the label"
   ]
  },
  {
   "cell_type": "code",
   "id": "6613bdae8241cf3b",
   "metadata": {
    "trusted": false
   },
   "source": "client.delete_job(name=\"sft-rag\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f94a849b67cec9f",
   "metadata": {
    "trusted": false
   },
   "source": [
    "client.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=\"sft-rag\",\n",
    "    train_func=main,\n",
    "    labels={\n",
    "        \"kueue.x-k8s.io/queue-name\" : \"\",\n",
    "    },\n",
    "    num_workers=3,\n",
    "    num_procs_per_worker=\"1\",\n",
    "    resources_per_worker={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"memory\": \"64Gi\",\n",
    "        \"cpu\": 8,\n",
    "    },\n",
    "    base_image=\"quay.io/modh/training:py311-cuda124-torch251\",\n",
    "    env_vars={\n",
    "        \"CUDA_LAUNCH_BLOCKING\": \"1\",\n",
    "        \"HF_HOME\": \"/mnt/shared/.cache\",\n",
    "        \"HF_TOKEN\": \"\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"PYTORCH_HIP_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"TRANSFORMERS_VERBOSITY\": \"info\",\n",
    "    },\n",
    "    parameters=parameters,\n",
    "    volumes=[\n",
    "        V1Volume(name=\"shared\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"shared\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"shared\", mount_path=\"/mnt/shared\"),\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83ec2fde840c991c",
   "metadata": {},
   "source": "llOnce the training job is created, you can follow its progress:"
  },
  {
   "cell_type": "code",
   "id": "bc16a7d9a7e036c4",
   "metadata": {
    "trusted": false
   },
   "source": "client.get_job_logs(\n    name=\"sft-rag\",\n    job_kind=\"PyTorchJob\",\n    follow=True,\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a7947779c815ba8",
   "metadata": {},
   "source": "# TensorBoard Setup\nYou can track your job runs and visualize the training metrics with TensorBoard. Enable TensorBoard logging for real-time visualization of training metrics, such as loss curves and ROUGE scores, to better monitor model learning behavior."
  },
  {
   "cell_type": "code",
   "id": "9a40643d2a79f62e",
   "metadata": {
    "trusted": false
   },
   "source": "import os\nos.environ[\"TENSORBOARD_PROXY_URL\"]= os.environ[\"NB_PREFIX\"]+\"/proxy/6006/\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "753e3c4484b700c0",
   "metadata": {
    "trusted": false
   },
   "source": "%load_ext tensorboard",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e2c9f38cdf2d00f",
   "metadata": {
    "trusted": false
   },
   "source": "%tensorboard --logdir /opt/app-root/src/shared",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db6c045c05943cb5",
   "metadata": {},
   "source": "## Inference: Testing the Fine-Tuned RAG Model\nAfter fine-tuning, we load the fine-tuned RAG model and run inference on test queries.\nThe `RagModel` will internally perform retrieval and then generate an answer based on the retrieved context."
  },
  {
   "cell_type": "code",
   "id": "bd71543a79a27d5d",
   "metadata": {
    "trusted": false
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import (\n",
    "    RagSequenceForGeneration,\n",
    "    RagConfig,\n",
    "    RagTokenizer,\n",
    "    DPRQuestionEncoder,\n",
    ")\n",
    "\n",
    "# Add feature store module path\n",
    "CUSTOM_MODULES_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag\"\n",
    "sys.path.append(CUSTOM_MODULES_PATH)\n",
    "\n",
    "# Import custom retriever and feature view definitions\n",
    "from feast.rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "# Define paths for Feast store and fine-tuned model path\n",
    "STORE_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag/feature_repo\"\n",
    "FINETUNED_RAG_CHECKPOINT_DIR = \"/opt/app-root/src/shared/fine_tuned_rag_model/inference_sft-rag-master-0\"\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load RAG configuration from fine-tuned model path\n",
    "rag_config_inference = RagConfig.from_pretrained(FINETUNED_RAG_CHECKPOINT_DIR)\n",
    "\n",
    "# Load tokenizers from model save path\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(FINETUNED_RAG_CHECKPOINT_DIR)\n",
    "question_encoder_tokenizer = rag_tokenizer.question_encoder\n",
    "generator_tokenizer_inference = rag_tokenizer.generator\n",
    "\n",
    "# Load the original (frozen) question encoder\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ").to(device)\n",
    "question_encoder.eval()\n",
    "\n",
    "# Initialize Feast retriever with feature definitions\n",
    "print(\"Initializing custom FeastRAGRetriever...\")\n",
    "feast_index_inference = FeastIndex()\n",
    "\n",
    "features_to_retrieve = [\n",
    "    \"wiki_passages:passage_text\",\n",
    "    \"wiki_passages:embedding\",\n",
    "    \"wiki_passages:passage_id\",\n",
    "]\n",
    "\n",
    "# Build retriever\n",
    "rag_retriever_inference = FeastRAGRetriever(\n",
    "    question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "    generator_tokenizer=generator_tokenizer_inference,\n",
    "    question_encoder=None,\n",
    "    generator_model=None,\n",
    "    feast_repo_path=STORE_PATH,\n",
    "    feature_view=wiki_passage_feature_view,\n",
    "    features=features_to_retrieve,\n",
    "    search_type=\"vector\",\n",
    "    config=rag_config_inference,\n",
    "    index=feast_index_inference,\n",
    ")\n",
    "\n",
    "# Load fine-tuned RAG model and attach custom retriever\n",
    "print(f\"Loading full RagSequenceForGeneration model from: {FINETUNED_RAG_CHECKPOINT_DIR}\")\n",
    "finetuned_rag_model = RagSequenceForGeneration.from_pretrained(\n",
    "    FINETUNED_RAG_CHECKPOINT_DIR,\n",
    "    retriever=rag_retriever_inference,\n",
    ")\n",
    "\n",
    "# Inject the question encoder into the fine-tuned RAG model\n",
    "finetuned_rag_model.rag.question_encoder = question_encoder\n",
    "finetuned_rag_model.to(device)\n",
    "finetuned_rag_model.eval()\n",
    "\n",
    "# Set the question encoder inside the retriever\n",
    "rag_retriever_inference.question_encoder = finetuned_rag_model.rag.question_encoder\n",
    "\n",
    "print(f\"Fine-tuned RAG model loaded and moved to {device}. Ready for inference.\")\n",
    "\n",
    "# Example queries to test inference\n",
    "test_queries = [\n",
    "    \"What is the boiling point of water in Celsius?\",\n",
    "    \"What is the capital city of Australia?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"Who wrote 'Alice's Adventures in Wonderland'?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"What is the function of the kidneys in the human body?\",\n",
    "]\n",
    "\n",
    "# Inference loop for test queries\n",
    "for test_query in test_queries:\n",
    "    print(f\"\\nQuery: {test_query}\")\n",
    "    try:\n",
    "        # Tokenize the query\n",
    "        inputs = question_encoder_tokenizer(\n",
    "            test_query,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate answer using the RAG model\n",
    "        with torch.no_grad():\n",
    "            generated_ids = finetuned_rag_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "\n",
    "        # Decode generated token IDs to text\n",
    "        answer = generator_tokenizer_inference.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Answer: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for query '{test_query}': {e}\")\n",
    "        raise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e38fca67ef241b6b",
   "metadata": {},
   "source": "# Cleaning Up"
  },
  {
   "cell_type": "markdown",
   "id": "c49aa31b460610ea",
   "metadata": {},
   "source": "## Delete Training Job\nOnce you're done or want to re-create the training job, you can delete the existing one:"
  },
  {
   "cell_type": "code",
   "id": "46d89953dee331ea",
   "metadata": {
    "trusted": false
   },
   "source": "client.delete_job(name=\"sft-rag\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d26740e5690edec",
   "metadata": {},
   "source": "## GPU Memory\nIf you want to start over and test the pre-trained model again, you can free the GPU / accelerator memory with:"
  },
  {
   "cell_type": "code",
   "id": "b6c31b4aa9a2116b",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "\n",
    "del finetuned_rag_model, rag_retriever_inference, rag_config_inference\n",
    "del generator_tokenizer_inference, question_encoder_tokenizer, feast_index_inference, question_encoder\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

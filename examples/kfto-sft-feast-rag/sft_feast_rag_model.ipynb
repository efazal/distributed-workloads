{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da333c8bd896040",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbc4d625275778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the YAML magic\n",
    "%pip install yamlmagic\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b26c61436737ee",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "Edit the following training hyperparameters and can be modified to experiment with learning rates, batch\\ sizes, LoRA parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2cab9c84e4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Model\n",
    "model_name_or_path: facebook/bart-large    # only works with Encoder-Decoder models\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: eager                # one of eager (default), sdpa or flash_attention_2\n",
    "use_liger: false                          # use Liger kernels\n",
    "\n",
    "# PEFT / LoRA (Apply to Generator Model)\n",
    "use_peft: false\n",
    "lora_r: 16\n",
    "lora_alpha: 8\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]   # Ensure these match your generator model\n",
    "lora_modules_to_save: []\n",
    "\n",
    "# QLoRA (BitsAndBytes) (Apply to Generator Model)\n",
    "load_in_4bit: false                       # use 4 bit precision for the base model (only with LoRA)\n",
    "load_in_8bit: false                       # use 8 bit precision for the base model (only with LoRA)\n",
    "\n",
    "# Dataset\n",
    "dataset_name: facebook/wiki_dpr\n",
    "dataset_config: main                      # name of the dataset configuration\n",
    "dataset_train_split: train                # dataset split to use for training (for RAG generated data)\n",
    "dataset_test_split: test                  # dataset split to use for evaluation (for RAG generated data)\n",
    "dataset_kwargs:\n",
    "    add_special_tokens: false               # template with special tokens\n",
    "    append_concat_token: false              # add additional separator token\n",
    "\n",
    "# SFT (These parameters will now apply to the RagModel's training)\n",
    "max_seq_length: 1024                      # max sequence length for model and packing of the dataset\n",
    "dataset_batch_size: 1000                  # samples to tokenize per batch (for initial data processing)\n",
    "packing: false                            # Packing is generally not used directly with RagModel training in the same way as SFT\n",
    "\n",
    "# Training\n",
    "num_train_epochs: 3                       # number of training epochs\n",
    "remove_unused_columns: false\n",
    "label_smoothing_factor: 0.1                # 0.1, 0.0(disable)\n",
    "\n",
    "per_device_train_batch_size: 1            # Batch size per device during training\n",
    "per_device_eval_batch_size: 1             # Batch size for evaluation\n",
    "auto_find_batch_size: false               # find a batch size that fits into memory automatically\n",
    "eval_strategy: epoch                      # evaluate every epoch\n",
    "\n",
    "bf16: true                                # use bf16 16-bit (mixed) precision\n",
    "tf32: true                               # use tf32 precision\n",
    "\n",
    "learning_rate: 4.0e-6                     # 4.0e-6 Initial learning rate for RAG model training\n",
    "warmup_steps: 200                         # steps for a linear warmup from 0 to `learning_rate`\n",
    "lr_scheduler_type: cosine                 # learning rate scheduler (see transformers.SchedulerType)\n",
    "\n",
    "optim: adamw_torch_fused                  # optimizer (see transformers.OptimizerNames)\n",
    "max_grad_norm: 1.0                        # max gradient norm\n",
    "seed: 42\n",
    "\n",
    "gradient_accumulation_steps: 8            # Increase for smaller per_device_train_batch_size\n",
    "gradient_checkpointing: false             # use gradient checkpointing to save memory\n",
    "gradient_checkpointing_kwargs:\n",
    "    use_reentrant: false\n",
    "\n",
    "# FSDP\n",
    "fsdp: \"full_shard auto_wrap\"              # add offload if not enough GPU memory\n",
    "# fsdp: \"\"\n",
    "fsdp_config:\n",
    "    activation_checkpointing: true\n",
    "    cpu_ram_efficient_loading: false\n",
    "    sync_module_states: true\n",
    "    use_orig_params: true\n",
    "    limit_all_gathers: false\n",
    "# fsdp_transformer_layer_cls_to_wrap: [BertLayer, BartEncoderLayer, BartDecoderLayer]\n",
    "\n",
    "# Checkpointing\n",
    "save_strategy: epoch                      # save checkpoint every epoch\n",
    "save_total_limit: 1                       # limit the total amount of checkpoints\n",
    "resume_from_checkpoint: false             # load the last checkpoint in output_dir and resume from it\n",
    "\n",
    "# Logging\n",
    "log_level: warning                        # logging level (see transformers.logging)\n",
    "logging_strategy: steps\n",
    "logging_steps: 1                          # log every N steps\n",
    "report_to:\n",
    "- tensorboard                             # report metrics to tensorboard\n",
    "\n",
    "output_dir: /mnt/shared/fine_tuned_rag_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49344963140de",
   "metadata": {},
   "source": [
    "# Feast Setup with Milvus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f28a4-1c18-488a-ad74-a8c857ebd0f1",
   "metadata": {},
   "source": [
    "### Install Required Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aac78970e46f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --quiet feast[milvus] sentence-transformers datasets faiss-cpu\n",
    "pip install bigtree==0.19.2\n",
    "pip install marshmallow==3.10.0\n",
    "pip install git+https://github.com/feast-dev/feast.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42b594-0ada-4ef8-8922-daed60745d2b",
   "metadata": {},
   "source": [
    "## Loading Wikipedia Dataset\n",
    "We only load a subset of the dataset in the interest of keeping this example runnable with minimum memory and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e72e9-8410-4775-961d-0ec1501e74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load wikipedia dataset - 1% of the training split\n",
    "dataset = load_dataset(\n",
    "    \"facebook/wiki_dpr\",\n",
    "    \"psgs_w100.nq.exact\",\n",
    "    split=\"train[:1%]\",\n",
    "    with_index=False,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e5640-e71d-45d4-8acf-026209b4309b",
   "metadata": {},
   "source": [
    "## Chunking Wikipedia Dataset\n",
    "The dataset is chunked to contain a preset number of chars, which is the max supported by Feast. Ensuring the chunk only contains whole words, thus the retrieved context can form sentences without incomplete words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c9ec70c155da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dataset(examples, max_chars=380):\n",
    "    all_chunks = []\n",
    "    all_ids = []\n",
    "    all_titles = []\n",
    "\n",
    "    for i, text in enumerate(examples['text']): # Iterate over texts in the batch\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        current_chunk_words = []\n",
    "        for word in words:\n",
    "            # Check if adding the next word exceeds the character limit\n",
    "            if len(' '.join(current_chunk_words + [word])) > max_chars:\n",
    "                # If the current chunk is valid, save it\n",
    "                if current_chunk_words:\n",
    "                    chunk_text = ' '.join(current_chunk_words)\n",
    "                    all_chunks.append(chunk_text)\n",
    "                    all_ids.append(f\"{examples['id'][i]}_{len(all_chunks)}\") # Unique ID for the chunk\n",
    "                    all_titles.append(examples['title'][i])\n",
    "                # Start a new chunk with the current word\n",
    "                current_chunk_words = [word]\n",
    "            else:\n",
    "                current_chunk_words.append(word)\n",
    "\n",
    "        # Add the last remaining chunk\n",
    "        if current_chunk_words:\n",
    "            chunk_text = ' '.join(current_chunk_words)\n",
    "            all_chunks.append(chunk_text)\n",
    "            all_ids.append(f\"{examples['id'][i]}_{len(all_chunks)}\") # Unique ID for the chunk\n",
    "            all_titles.append(examples['title'][i])\n",
    "\n",
    "    return {'id': all_ids, 'title': all_titles, 'text': all_chunks}\n",
    "\n",
    "\n",
    "chunked_dataset = dataset.map(\n",
    "    chunk_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b7551-4950-4eb4-aa5b-7d658a0effc7",
   "metadata": {},
   "source": [
    "### Create DPR Embeddings\n",
    "We load a pre-trained Dense Passage Retrieval (DPR) encoder to generate context embeddings for each chunked passage. These embeddings will later be stored in the Feast feature store and queried during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc319fd9aa1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load DPR Context Encoder model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(embedding_model_name)\n",
    "model = DPRContextEncoder.from_pretrained(embedding_model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "sentences = chunked_dataset[\"text\"]\n",
    "\n",
    "print(f\"Generating DPR embeddings for {len(sentences)} documents...\")\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(sentences), 16): # Process in batches\n",
    "        batch_texts = sentences[i:i+16]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"].to(device)\n",
    "        embeddings = model(inputs).pooler_output\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Embeddings generated with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d3715-0088-4439-8d5b-deb477ecb8c8",
   "metadata": {},
   "source": [
    "## Create Parquet File for Feast Offline Store\n",
    "Create a parquet file using the DPR embeddings created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d4b0b35f7eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"passage_id\": list(range(len(sentences))),\n",
    "    \"passage_text\": sentences,\n",
    "    \"embedding\": pd.Series(\n",
    "        [embedding.tolist() for embedding in embeddings],\n",
    "        dtype=object\n",
    "    ),\n",
    "    \"event_timestamp\": [datetime.now(timezone.utc) for _ in sentences],\n",
    "})\n",
    "\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.head())\n",
    "print(df[\"embedding\"].apply(lambda x: len(x) if isinstance(x, list) else str(type(x))).value_counts())  # Check lengths\n",
    "\n",
    "# Save to Parquet\n",
    "df.to_parquet(\"feature_repo/wiki_dpr.parquet\", index=False)\n",
    "print(\"Saved to wiki_dpr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f8b2c2c50ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd feature_repo"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Feast Feature Repository",
   "id": "b7a775e6b6fac1d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d6a3af2085f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35cb27-fe51-4f1b-ab48-fb41c0fcb7d7",
   "metadata": {},
   "source": [
    "## Writing to Feast Online Store (Milvus)\n",
    "We load the Parquet file into Milvus via Feast, which will serve as the online store for efficient similarity search during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f8fc7be3137bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "df = pd.read_parquet(\"./wiki_dpr.parquet\")\n",
    "chunk_size = 10000\n",
    "num_rows = len(df)\n",
    "\n",
    "for i in range(0, num_rows, chunk_size):\n",
    "    chunk_df = df.iloc[i:i + chunk_size]\n",
    "    print(f\"Writing chunk {i//chunk_size + 1}/{(num_rows + chunk_size - 1)//chunk_size}...\")\n",
    "    store.write_to_online_store(feature_view_name='wiki_passages', df=chunk_df)\n",
    "    print(\"Chunk written successfully.\")\n",
    "\n",
    "print(\"All data written to online store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf154268-846e-4d9c-8586-7d260dc30106",
   "metadata": {},
   "source": [
    "# Preprocessing Natural Questions Dataset for RAG Training\n",
    "We prepare the Natural Questions dataset for RAG model training. The Dataset contains Questions and Answers. The dataset is quite big, and again in the interest of keepig this example runnable, we are using a subset of training & validation samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d69b21221a99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "from transformers import DPRQuestionEncoderTokenizer, DPRQuestionEncoder, AutoTokenizer\n",
    "\n",
    "print(\"Preparing training data (Q&A pairs) with caching...\")\n",
    "processed_data_cache_dir = \"dataset/rag_synthetic_qa_dataset\"\n",
    "Path(processed_data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load question encoder tokenizer and model\n",
    "question_encoder_model_name_or_path = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_encoder_model_name_or_path)\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    question_encoder_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Load generator tokenizer\n",
    "generator_model_name_or_path = \"facebook/bart-large\"\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    generator_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Filter function to retain only examples with valid short answers\n",
    "def has_valid_answer(answer):\n",
    "    if not answer[\"annotations\"][\"short_answers\"]:\n",
    "        return False\n",
    "\n",
    "    for short_ans_dict in answer[\"annotations\"][\"short_answers\"]:\n",
    "        if (\"text\" in short_ans_dict and\n",
    "                short_ans_dict[\"text\"] and\n",
    "                short_ans_dict[\"text\"][0].strip()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Preprocessing function to tokenize question and answer for RAG input\n",
    "def preprocess_nq_example(example):\n",
    "    question_text = example[\"question\"][\"text\"]\n",
    "    answer_text = \"\"\n",
    "\n",
    "    # Select the first available short answer\n",
    "    if example[\"annotations\"][\"short_answers\"]:\n",
    "        for short_ans_dict in example[\"annotations\"][\"short_answers\"]:\n",
    "            if \"text\" in short_ans_dict and short_ans_dict[\"text\"] and short_ans_dict[\"text\"][0].strip():\n",
    "                answer_text = short_ans_dict[\"text\"][0]\n",
    "\n",
    "    # Tokenize question for the RAG model's question encoder\n",
    "    tokenized_question = question_encoder_tokenizer(\n",
    "        question_text,\n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize answer for the RAG model's generator\n",
    "    tokenized_answer_for_labels = generator_tokenizer(\n",
    "        text_target=answer_text,\n",
    "        truncation=True,\n",
    "        max_length=28,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_question[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_question[\"attention_mask\"],\n",
    "        \"labels\": tokenized_answer_for_labels[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "nq_dataset_name = \"natural_questions\"\n",
    "nq_train_split = \"train\"\n",
    "nq_validation_split = \"validation\"\n",
    "\n",
    "# Check if preprocessed datasets are cached\n",
    "if (Path(processed_data_cache_dir) / nq_train_split).exists() and \\\n",
    "        (Path(processed_data_cache_dir) / nq_validation_split).exists():\n",
    "    print(f\"Loading preprocessed data from cache: {processed_data_cache_dir}\")\n",
    "    loaded_processed_datasets = load_from_disk(processed_data_cache_dir)\n",
    "    train_dataset = loaded_processed_datasets[\"train\"]\n",
    "    test_dataset = loaded_processed_datasets[\"validation\"]\n",
    "\n",
    "else:\n",
    "    print(\"Loading raw Natural Questions dataset (streaming) and preprocessing...\")\n",
    "\n",
    "    num_train_samples = 2000  # Target number of valid training samples\n",
    "    num_eval_samples = 200    # Target number of valid evaluation samples\n",
    "\n",
    "    raw_train_stream = load_dataset(nq_dataset_name, split=nq_train_split, streaming=True)\n",
    "    raw_eval_stream = load_dataset(nq_dataset_name, split=nq_validation_split, streaming=True)\n",
    "\n",
    "    # Stream and filter training data on the fly until target count is reached\n",
    "    temp_raw_train_list = []\n",
    "\n",
    "    for example in raw_train_stream:\n",
    "        if has_valid_answer(example):\n",
    "            temp_raw_train_list.append(example)\n",
    "            if len(temp_raw_train_list) >= num_train_samples:\n",
    "                break\n",
    "\n",
    "    # Stream and filter evaluation data on the fly until target count is reached\n",
    "    temp_raw_eval_list = []\n",
    "    for example in raw_eval_stream:\n",
    "        if has_valid_answer(example):\n",
    "            temp_raw_eval_list.append(example)\n",
    "            if len(temp_raw_eval_list) >= num_eval_samples:\n",
    "                break\n",
    "\n",
    "    raw_datasets_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_list(temp_raw_train_list),\n",
    "        \"validation\": Dataset.from_list(temp_raw_eval_list)\n",
    "    })\n",
    "\n",
    "    print(\"Applying preprocessing to filtered samples...\")\n",
    "    processed_datasets = raw_datasets_dict.map(\n",
    "        preprocess_nq_example,\n",
    "        remove_columns=raw_datasets_dict[\"train\"].column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    test_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "    print(f\"Saving preprocessed datasets to cache: {processed_data_cache_dir}\")\n",
    "    processed_datasets.save_to_disk(processed_data_cache_dir)\n",
    "    print(\"Preprocessed data saved to cache.\")\n",
    "\n",
    "# Dataset summary\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train dataset columns: {train_dataset.column_names}\")\n",
    "print(f\"Sample from train dataset: {train_dataset[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879b1d8-5086-449a-8872-6e79d705d97a",
   "metadata": {},
   "source": [
    "### Dataset Sanity Check\n",
    "Randomly inspect samples from the preprocessed dataset to ensure questions and answers are correctly tokenized and properly structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb491790cde0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for index in random.sample(range(len(train_dataset)), min(3, len(train_dataset))):\n",
    "    print(f\"\\n  Processed Sample {index}  \")\n",
    "    print(f\"Question: {train_dataset[index].keys()}\")\n",
    "    print(f\"Decoded Question: {question_encoder_tokenizer.decode(train_dataset[index]['input_ids'], skip_special_tokens=True)}\")\n",
    "    print(f\"Decoded Labels (Answer): {generator_tokenizer.decode(train_dataset[index]['labels'], skip_special_tokens=True)}\")\n",
    "    print(f\"Raw Input IDs (Question): {train_dataset[index]['input_ids'][:20]}...\")\n",
    "    print(f\"Raw Labels (Answer): {train_dataset[index]['labels'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4473b366d7156b5",
   "metadata": {},
   "source": [
    "### Preparing Training Assets for Distributed Training\n",
    "\n",
    "Copy over the `kfto-sft-feast-rag` folder into the shared storage, so that it can be accessed by the training job, since the FeastRagRetriever requires the `feature_repo` directory during training and inference. Copy over the `feast_rag_retriever.py` script as well, so that it can be imported in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48069de28ffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r $HOME/distributed-workloads/examples/kfto-sft-feast-rag $HOME/shared/\n",
    "cp $HOME/distributed-workloads/examples/kfto_feast_rag/feast_rag_retriever.py $HOME/shared/kfto-sft-feast-rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c88f6d4a5099b1",
   "metadata": {},
   "source": [
    "# Fine-tuning the RAG Model (Training Loop)\n",
    "We initiate the fine-tuning process for the RAG model using the Seq2SeqTrainer. This step integrates the retriever, encoder, generator, and custom datasets to jointly optimize the generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97070e5ea5bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(parameters):\n",
    "    import subprocess, sys\n",
    "    # Install necessary packages\n",
    "    # This ensures Feast and other dependencies are available inside the container\n",
    "    print(\"Installing Feast and other RAG dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"feast[milvus]\", \"sentence-transformers\", \"datasets\", \"bigtree==0.19.2\", \"marshmallow==3.10.0\", \"protobuf>=3.10.0\", \"git+https://github.com/feast-dev/feast.git@master\", \"faiss-cpu\", \"evaluate\", \"rouge_score\", \"nltk\"])\n",
    "    print(\"Feast and other RAG dependencies installed.\")\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        set_seed,\n",
    "        RagSequenceForGeneration,\n",
    "        GenerationConfig,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        Seq2SeqTrainer,\n",
    "        RagTokenizer,\n",
    "        default_data_collator,\n",
    "        RagConfig,\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        DPRQuestionEncoderTokenizer,\n",
    "        DPRQuestionEncoder\n",
    "    )\n",
    "    from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "    import torch\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import evaluate\n",
    "\n",
    "    from trl import (\n",
    "        ModelConfig,\n",
    "        ScriptArguments,\n",
    "        SFTConfig,\n",
    "        TrlParser,\n",
    "        get_peft_config,\n",
    "    )\n",
    "\n",
    "    # This is required for `feast_rag_retriever` to be found (This is temporary until Feast RAG Retriever is part of Feast SDK)\n",
    "    CUSTOM_MODULES_PATH = \"/mnt/shared/kfto-sft-feast-rag\"\n",
    "    sys.path.append(CUSTOM_MODULES_PATH)\n",
    "    print(f\"Added {CUSTOM_MODULES_PATH} to sys.path for custom module imports.\")\n",
    "    from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "    from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args_trl, model_args = parser.parse_dict(parameters)\n",
    "\n",
    "    # Convert SFTConfig parameters to standard TrainingArguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=training_args_trl.output_dir,\n",
    "        num_train_epochs=training_args_trl.num_train_epochs,\n",
    "        per_device_train_batch_size=training_args_trl.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_args_trl.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=training_args_trl.gradient_accumulation_steps,\n",
    "        gradient_checkpointing=training_args_trl.gradient_checkpointing,\n",
    "        learning_rate=training_args_trl.learning_rate,\n",
    "        warmup_steps=training_args_trl.warmup_steps,\n",
    "        lr_scheduler_type=training_args_trl.lr_scheduler_type,\n",
    "        optim=training_args_trl.optim,\n",
    "        max_grad_norm=training_args_trl.max_grad_norm,\n",
    "        seed=training_args_trl.seed,\n",
    "        bf16=training_args_trl.bf16,\n",
    "        tf32=training_args_trl.tf32,\n",
    "        eval_strategy=training_args_trl.eval_strategy,\n",
    "        save_strategy=training_args_trl.save_strategy,\n",
    "        save_total_limit=training_args_trl.save_total_limit,\n",
    "        logging_strategy=training_args_trl.logging_strategy,\n",
    "        logging_steps=training_args_trl.logging_steps,\n",
    "        report_to=training_args_trl.report_to,\n",
    "        fsdp=training_args_trl.fsdp if hasattr(training_args_trl, 'fsdp') else None,\n",
    "        fsdp_config=training_args_trl.fsdp_config if hasattr(training_args_trl, 'fsdp_config') else None,\n",
    "        resume_from_checkpoint=training_args_trl.resume_from_checkpoint,\n",
    "        remove_unused_columns=training_args_trl.remove_unused_columns,\n",
    "        predict_with_generate= True,\n",
    "    )\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Question Encoder\n",
    "    question_encoder_model_name_or_path = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "    question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_encoder_model_name_or_path)\n",
    "    question_encoder_model = DPRQuestionEncoder.from_pretrained(\n",
    "        question_encoder_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        torch_dtype=model_args.torch_dtype\n",
    "    )\n",
    "\n",
    "    # Generator Model to be fine-tuned\n",
    "    generator_model_name_or_path = parameters[\"model_name_or_path\"]\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        use_fast=True\n",
    "    )\n",
    "    generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        generator_model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=model_args.torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    )\n",
    "\n",
    "    if model_args.use_peft:\n",
    "        print(f\"PEFT/LoRA: Ensure 'lora_target_modules' in your parameters ({parameters.get('lora_target_modules')}) are correct for the new generator model '{generator_model_name_or_path}'.\")\n",
    "        generator_model = prepare_model_for_kbit_training(generator_model)\n",
    "        peft_config = get_peft_config(model_args)\n",
    "        generator_model = get_peft_model(generator_model, peft_config)\n",
    "        print(\"PEFT setup for generator model completed.\")\n",
    "\n",
    "    store_path = CUSTOM_MODULES_PATH+\"/feature_repo\"\n",
    "\n",
    "    # Initialize FeastRAGRetriever\n",
    "    feast_index = FeastIndex()\n",
    "    rag_top_k = 10\n",
    "\n",
    "    question_encoder_config = {\n",
    "        \"model_type\": \"dpr\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"vocab_size\": question_encoder_tokenizer.vocab_size,\n",
    "        \"num_hidden_layers\": 6,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"projection_dim\": 0,\n",
    "        \"torch_dtype\": model_args.torch_dtype\n",
    "    }\n",
    "\n",
    "    rag_config = RagConfig(\n",
    "        question_encoder=question_encoder_config,\n",
    "        generator=generator_model.config.to_dict(),\n",
    "        index_name=\"custom\",\n",
    "        index=feast_index,\n",
    "        n_docs=rag_top_k\n",
    "    )\n",
    "\n",
    "    features_to_retrieve = [\n",
    "        \"wiki_passages:passage_text\",\n",
    "        \"wiki_passages:embedding\",\n",
    "        \"wiki_passages:passage_id\",\n",
    "    ]\n",
    "\n",
    "    rag_retriever = FeastRAGRetriever(\n",
    "        question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "        generator_tokenizer=generator_tokenizer,\n",
    "        feast_repo_path=store_path,\n",
    "        feature_view=wiki_passage_feature_view,\n",
    "        features=features_to_retrieve,\n",
    "        search_type=\"vector\",\n",
    "        config=rag_config,\n",
    "        index=feast_index,\n",
    "    )\n",
    "\n",
    "    # Initialize the RagModel for fine-tuning\n",
    "    model = RagSequenceForGeneration(\n",
    "        question_encoder=question_encoder_model,\n",
    "        config=rag_config,\n",
    "        generator=generator_model,   # model being fine-tuned\n",
    "        retriever=rag_retriever\n",
    "    )\n",
    "    generator_config = GenerationConfig(\n",
    "        max_length=128,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "    model.generation_config = generator_config\n",
    "    # Train the question encoder and generator together, False means don't train question encoder\n",
    "    # model.rag.question_encoder.requires_grad_(False)\n",
    "    # for param in model.rag.question_encoder.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    print(\"RAG Model initialized.\")\n",
    "\n",
    "    # Explicitly move model to GPU\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load preprocessed NQ dataset\n",
    "    print(\"Preparing training data (Q&A pairs) with caching...\")\n",
    "    processed_data_cache_dir = \"/mnt/shared/kfto-sft-feast-rag/dataset/rag_synthetic_qa_dataset\"\n",
    "    Path(processed_data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nq_train_split = \"train\"\n",
    "    nq_validation_split = \"validation\"\n",
    "    # Check if the preprocessed dataset is already cached\n",
    "    if (Path(processed_data_cache_dir) / nq_train_split).exists() and \\\n",
    "            (Path(processed_data_cache_dir) / nq_validation_split).exists():\n",
    "        print(f\"Loading preprocessed data from cache: {processed_data_cache_dir}\")\n",
    "        loaded_processed_datasets = load_from_disk(processed_data_cache_dir)\n",
    "        train_dataset = loaded_processed_datasets[\"train\"]\n",
    "        test_dataset = loaded_processed_datasets[\"validation\"]\n",
    "    else:\n",
    "        print(\"ERROR DATASET NOT FOUND\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    def compute_semantic_similarity(preds, labels):\n",
    "        semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        pred_embeddings = semantic_model.encode(preds, convert_to_tensor=True)\n",
    "        label_embeddings = semantic_model.encode(labels, convert_to_tensor=True)\n",
    "\n",
    "        cosine_similarities = util.cos_sim(pred_embeddings, label_embeddings)\n",
    "        cosine_similarities = cosine_similarities.diag()  # Get similarities of matching pairs\n",
    "\n",
    "        avg_similarity = cosine_similarities.mean().item() * 100\n",
    "        return avg_similarity\n",
    "\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        predictions = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "\n",
    "        labels = np.where(labels != -100, labels, generator_tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_predictions = generator_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = generator_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Clean and normalize outputs\n",
    "        decoded_predictions = [pred.strip().lower() for pred in decoded_predictions]\n",
    "        decoded_labels = [label.strip().lower() for label in decoded_labels]\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        results = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "        # Calculate Exact Match\n",
    "        exact_matches = [\n",
    "            int(pred == label) for pred, label in zip(decoded_predictions, decoded_labels)\n",
    "        ]\n",
    "        exact_match_score = np.mean(exact_matches) * 100\n",
    "        semantic_score = compute_semantic_similarity(decoded_predictions, decoded_labels)\n",
    "\n",
    "        return {\n",
    "            \"rouge1\": results[\"rouge1\"] * 100,\n",
    "            \"rouge2\": results[\"rouge2\"] * 100,\n",
    "            \"rougeL\": results[\"rougeL\"] * 100,\n",
    "            \"rougeLsum\": results[\"rougeLsum\"] * 100,\n",
    "            \"exact_match\": exact_match_score,\n",
    "            \"semantic_similarity\": semantic_score,\n",
    "        }\n",
    "\n",
    "    # Training the RagModel\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=generator_tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        if hasattr(trainer.model.generator, \"print_trainable_parameters\"):\n",
    "            print(\"Trainable parameters for PEFT-enabled generator:\")\n",
    "            trainer.model.generator.print_trainable_parameters()\n",
    "        else:\n",
    "            print(\"Trainer model does not have 'print_trainable_parameters' method on its generator.\")\n",
    "\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    # Ensure RAG model forward pass works before training\n",
    "    print(\"Test forward pass prior to training\")\n",
    "    example_query_for_test = \"What did the socrates say?\"\n",
    "    example_answer_for_test = \"They said great things\"\n",
    "    print(f\"Query: {example_query_for_test}\")\n",
    "    question_tok = question_encoder_tokenizer(example_query_for_test, return_tensors=\"pt\")\n",
    "    label_tok = generator_tokenizer(text_target=example_answer_for_test, return_tensors=\"pt\")\n",
    "\n",
    "    test_input = {\n",
    "        \"input_ids\": question_tok[\"input_ids\"].to('cuda'),\n",
    "        \"attention_mask\": question_tok[\"attention_mask\"].to('cuda'),\n",
    "        \"labels\": label_tok[\"input_ids\"].to('cuda')\n",
    "    }\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**test_input)\n",
    "        print(\"Test forward pass successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Starting RAG model training...\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"RAG model training completed.\")\n",
    "\n",
    "\n",
    "    # Save the Fine-tuned RAG Model\n",
    "    final_save_path = os.path.join(training_args.output_dir, \"inference_\" + os.getenv(\"HOSTNAME\", \"rag_model\"))\n",
    "\n",
    "    print(f\"Saving main model weights and RagConfig to: {final_save_path}\")\n",
    "    trainer.save_model(final_save_path)\n",
    "\n",
    "    if trainer.args.process_index == 0:\n",
    "        print(\"Manually saving component models and tokenizers to subdirectories...\")\n",
    "\n",
    "        # Unwrap the model to get access to its components\n",
    "        unwrapped_model = trainer.accelerator.unwrap_model(trainer.model)\n",
    "\n",
    "        # Save the question encoder to its subdirectory\n",
    "        qe_path = os.path.join(final_save_path, \"question_encoder\")\n",
    "        unwrapped_model.rag.question_encoder.save_pretrained(qe_path)\n",
    "        print(f\"Question encoder component saved to {qe_path}\")\n",
    "\n",
    "        # Save the generator to its subdirectory\n",
    "        gen_path = os.path.join(final_save_path, \"generator\")\n",
    "        unwrapped_model.rag.generator.save_pretrained(gen_path)\n",
    "        print(f\"Generator component saved to {gen_path}\")\n",
    "\n",
    "        # Save the RagTokenizer\n",
    "        rag_tokenizer_to_save = RagTokenizer(\n",
    "            question_encoder=question_encoder_tokenizer,\n",
    "            generator=generator_tokenizer\n",
    "        )\n",
    "        rag_tokenizer_to_save.save_pretrained(final_save_path)\n",
    "        print(f\"RagTokenizer components saved to subfolders in {final_save_path}\")\n",
    "\n",
    "    # Wait for all processes to finish before exiting\n",
    "    if trainer.is_fsdp_enabled or trainer.args.world_size > 1:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    print(f\"\\nTraining and saving complete. Your final, inference-ready model is at: {final_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285767a94abcd6a2",
   "metadata": {},
   "source": [
    "# Kubeflow Training Client\n",
    "Configure the Kubeflow SDK client by providing the required credentials to connect to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131fb67acb2267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kubeflow.training import TrainingClient\n",
    "from kubeflow.training.models import V1Volume, V1VolumeMount, V1PersistentVolumeClaimVolumeSource\n",
    "\n",
    "api_server = \"\"\n",
    "token = \"\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "api_client = client.ApiClient(configuration)\n",
    "client = TrainingClient(client_configuration=api_client.configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd4e0ba3924317",
   "metadata": {},
   "source": [
    "# Training Job\n",
    "You're now almost ready to create the training job:\n",
    "* Fill the `HF_TOKEN` environment variable with your HuggingFace token if you fine-tune a gated model\n",
    "* Check the number of worker nodes\n",
    "* Amend the resources per worker according to the job requirements\n",
    "* Update the PVC name to the one you've attached to the workbench if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613bdae8241cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(name=\"sft-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94a849b67cec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=\"sft-rag\",\n",
    "    train_func=main,\n",
    "    num_workers=3,\n",
    "    num_procs_per_worker=\"1\",\n",
    "    resources_per_worker={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"memory\": \"64Gi\",\n",
    "        \"cpu\": 8,\n",
    "    },\n",
    "    base_image=\"quay.io/modh/training:py311-cuda124-torch251\",\n",
    "    env_vars={\n",
    "        \"CUDA_LAUNCH_BLOCKING\": \"1\",\n",
    "        \"HF_HOME\": \"/mnt/shared/.cache\",\n",
    "        \"HF_TOKEN\": \"\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"PYTORCH_HIP_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"TRANSFORMERS_VERBOSITY\": \"info\",\n",
    "    },\n",
    "    parameters=parameters,\n",
    "    volumes=[\n",
    "        V1Volume(name=\"shared\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"shared\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"shared\", mount_path=\"/mnt/shared\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec2fde840c991c",
   "metadata": {},
   "source": [
    "llOnce the training job is created, you can follow its progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16a7d9a7e036c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_job_logs(\n",
    "    name=\"sft-rag\",\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    follow=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7947779c815ba8",
   "metadata": {},
   "source": [
    "# TensorBoard Setup\n",
    "You can track your job runs and visualize the training metrics with TensorBoard. Enable TensorBoard logging for real-time visualization of training metrics, such as loss curves and ROUGE scores, to better monitor model learning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40643d2a79f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TENSORBOARD_PROXY_URL\"]= os.environ[\"NB_PREFIX\"]+\"/proxy/6006/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e3c4484b700c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c9f38cdf2d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /opt/app-root/src/shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c045c05943cb5",
   "metadata": {},
   "source": [
    "## Inference: Testing the Fine-Tuned RAG Model\n",
    "After fine-tuning, we load the fine-tuned RAG model and run inference on test queries.\n",
    "The `RagModel` will internally perform retrieval and then generate an answer based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71543a79a27d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:09:12.339916Z",
     "start_time": "2025-06-23T15:09:07.156811Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import (\n",
    "    RagSequenceForGeneration,\n",
    "    RagConfig,\n",
    "    RagTokenizer,\n",
    "    DPRQuestionEncoder,\n",
    ")\n",
    "\n",
    "# Add custom Feast RAG Retriever module path (temporary workaround until properly packaged)\n",
    "CUSTOM_MODULES_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto_feast_rag\"\n",
    "sys.path.append(CUSTOM_MODULES_PATH)\n",
    "\n",
    "# Import custom retriever and feature view definitions\n",
    "from feast_rag_retriever import FeastRAGRetriever, FeastIndex\n",
    "from feature_repo.ragproject_repo import wiki_passage_feature_view\n",
    "\n",
    "# Define paths for Feast store and fine-tuned model checkpoint\n",
    "STORE_PATH = \"/opt/app-root/src/distributed-workloads/examples/kfto-sft-feast-rag/feature_repo\"\n",
    "FINETUNED_RAG_CHECKPOINT_DIR = \"/opt/app-root/src/shared/fine_tuned_rag_model/inference_sft-rag-master-0\"\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load RAG configuration from fine-tuned checkpoint\n",
    "rag_config_inference = RagConfig.from_pretrained(FINETUNED_RAG_CHECKPOINT_DIR)\n",
    "\n",
    "# Load tokenizers from checkpoint\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(FINETUNED_RAG_CHECKPOINT_DIR)\n",
    "question_encoder_tokenizer = rag_tokenizer.question_encoder\n",
    "generator_tokenizer_inference = rag_tokenizer.generator\n",
    "\n",
    "# Load the original (frozen) question encoder\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "question_encoder.to(device)\n",
    "question_encoder.eval()\n",
    "\n",
    "# Initialize Feast retriever with feature definitions\n",
    "print(\"Initializing custom FeastRAGRetriever...\")\n",
    "feast_index_inference = FeastIndex()\n",
    "\n",
    "features_to_retrieve = [\n",
    "    \"wiki_passages:passage_text\",\n",
    "    \"wiki_passages:embedding\",\n",
    "    \"wiki_passages:passage_id\",\n",
    "]\n",
    "\n",
    "# Build custom retriever (question_encoder is injected later)\n",
    "rag_retriever_inference = FeastRAGRetriever(\n",
    "    question_encoder_tokenizer=question_encoder_tokenizer,\n",
    "    generator_tokenizer=generator_tokenizer_inference,\n",
    "    question_encoder=None,\n",
    "    generator_model=None,\n",
    "    feast_repo_path=STORE_PATH,\n",
    "    feature_view=wiki_passage_feature_view,\n",
    "    features=features_to_retrieve,\n",
    "    search_type=\"vector\",\n",
    "    config=rag_config_inference,\n",
    "    index=feast_index_inference,\n",
    ")\n",
    "\n",
    "# Load fine-tuned RAG model and attach custom retriever\n",
    "print(f\"Loading full RagSequenceForGeneration model from: {FINETUNED_RAG_CHECKPOINT_DIR}\")\n",
    "finetuned_rag_model = RagSequenceForGeneration.from_pretrained(\n",
    "    FINETUNED_RAG_CHECKPOINT_DIR,\n",
    "    retriever=rag_retriever_inference,\n",
    ")\n",
    "\n",
    "# Inject the question encoder into the fine-tuned RAG model\n",
    "finetuned_rag_model.rag.question_encoder = question_encoder\n",
    "finetuned_rag_model.to(device)\n",
    "finetuned_rag_model.eval()\n",
    "\n",
    "# Set the question encoder inside the retriever\n",
    "rag_retriever_inference.question_encoder = finetuned_rag_model.rag.question_encoder\n",
    "\n",
    "print(f\"Fine-tuned RAG model loaded and moved to {device}. Ready for inference.\")\n",
    "\n",
    "# Example queries to test inference\n",
    "test_queries = [\n",
    "    \"Explain quantum theory.\",\n",
    "    \"What is the purpose of the United Nations?\",\n",
    "    \"Explain the concept of supply chain management.\",\n",
    "    \"Tell me about the history of the Pyramids of Giza.\",\n",
    "]\n",
    "\n",
    "# Inference loop for test queries\n",
    "for test_query in test_queries:\n",
    "    print(f\"\\nQuery: {test_query}\")\n",
    "    try:\n",
    "        # Tokenize the query\n",
    "        inputs = question_encoder_tokenizer(\n",
    "            test_query,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate answer using the RAG model\n",
    "        with torch.no_grad():\n",
    "            generated_ids = finetuned_rag_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "\n",
    "        # Decode generated token IDs to text\n",
    "        answer = generator_tokenizer_inference.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Answer: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for query '{test_query}': {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fca67ef241b6b",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49aa31b460610ea",
   "metadata": {},
   "source": [
    "## Delete Training Job\n",
    "Once you're done or want to re-create the training job, you can delete the existing one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d89953dee331ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(name=\"sft-rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26740e5690edec",
   "metadata": {},
   "source": [
    "## GPU Memory\n",
    "If you want to start over and test the pre-trained model again, you can free the GPU / accelerator memory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c31b4aa9a2116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "\n",
    "del finetuned_rag_model, rag_retriever_inference, rag_config_inference\n",
    "del question_encoder_tokenizer_inference, generator_tokenizer_inference, feast_index_inference, query_encoder_model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
